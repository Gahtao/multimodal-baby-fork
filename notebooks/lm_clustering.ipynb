{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/misc/vlgscratch4/LakeGroup/wentao/multimodal-baby\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torchvision.transforms.functional import resized_crop\n",
    "from multimodal.multimodal_lit import MultiModalLitModel\n",
    "from multimodal.multimodal_data_module import PAD_TOKEN_ID, UNK_TOKEN_ID, SOS_TOKEN_ID, EOS_TOKEN_ID, normalizer\n",
    "from ngram import NGramModel\n",
    "from analysis_tools.utils import *\n",
    "from analysis_tools.pos_tags import *\n",
    "from analysis_tools.word_categories import *\n",
    "from analysis_tools.token_items_data import *\n",
    "from analysis_tools.plotting import *\n",
    "import analysis_tools.plotting as plotting\n",
    "from analysis_tools.multimodal_visualization import *\n",
    "from analysis_tools.processing import *\n",
    "from analysis_tools.build_data import *\n",
    "from analysis_tools.checkpoints import *\n",
    "\n",
    "\n",
    "# set default settings for plotting; may change for each plot\n",
    "figsize = (8, 7)\n",
    "paper_context = sns.plotting_context('paper')\n",
    "paper_context.update({\n",
    "    'font.size': 10.,\n",
    "    'axes.labelsize': 10.,\n",
    "    'axes.titlesize': 14.,\n",
    "    'xtick.labelsize': 8.8,\n",
    "    'ytick.labelsize': 8.8,\n",
    "    'legend.fontsize': 8.8,\n",
    "    'legend.title_fontsize': 9.6,\n",
    "})\n",
    "unticked_relation_style = sns.axes_style('white')\n",
    "unticked_relation_style.update({\n",
    "    'axes.spines.left': True,\n",
    "    'axes.spines.bottom': True,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "})\n",
    "ticked_relation_style = sns.axes_style('ticks')\n",
    "ticked_relation_style.update({\n",
    "    'axes.spines.left': True,\n",
    "    'axes.spines.bottom': True,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "})\n",
    "heatmap_style = copy.copy(unticked_relation_style)\n",
    "heatmap_style.update({\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "})\n",
    "font = 'serif'\n",
    "sns.set_theme(\n",
    "    context=paper_context,\n",
    "    style=unticked_relation_style,\n",
    "    palette=sns.color_palette('tab20'),\n",
    "    font=font,\n",
    "    rc={\n",
    "        'figure.figsize': figsize,\n",
    "    }\n",
    ")\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=2, linewidth=120)\n",
    "pd.options.display.width = 120\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "plt.rcParams[\"savefig.pad_inches\"] = 0.\n",
    "plot_format = 'pdf'\n",
    "saving_fig = True\n",
    "\n",
    "if plot_format == 'pgf':\n",
    "    matplotlib.use('pgf')\n",
    "    plt.rcParams.update({\n",
    "        \"pgf.texsystem\": \"pdflatex\",\n",
    "        \"pgf.preamble\": \"\\n\".join([\n",
    "             r\"\\usepackage[T1]{fontenc}\",\n",
    "        ]),\n",
    "    })\n",
    "elif plot_format == 'svg':\n",
    "    from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "    set_matplotlib_formats('svg')\n",
    "\n",
    "if saving_fig:\n",
    "    def _save_fig(fname, format='png'):\n",
    "        print(f'saving plot {fname}')\n",
    "        plt.savefig(f'plots/{fname}.{format}', transparent=True)\n",
    "        plt.clf()\n",
    "    plotting.output_fig = functools.partial(_save_fig, format=plot_format)\n",
    "else:\n",
    "    plotting.output_fig = lambda fname: plt.show()\n",
    "\n",
    "output_fig = plotting.output_fig\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from checkpoints/lm_dataset_saycam_captioning_False_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_0/epoch=29.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:73: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using base transforms\n",
      "Calling prepare_data!\n",
      "SAYCam transcripts have already been downloaded. Skipping this step.\n",
      "Transcripts have already been renamed. Skipping this step.\n",
      "Transcripts have already been preprocessed. Skipping this step.\n",
      "Training frames have already been extracted. Skipping this step.\n",
      "Training metadata files have already been created. Skipping this step.\n",
      "Shuffled training metadata file has already been created. Skipping this step.\n",
      "Evaluation frames have already been filtered. Skipping this step.\n",
      "Evaluation frames have already been extracted. Skipping this step.\n",
      "Filtered evaluation frames have already been extracted. Skipping this step.\n",
      "Evaluation metadata files have already been created. Skipping this step.\n",
      "Evaluation metadata files have already been created. Skipping this step.\n",
      "Extra evaluation metadata files have already been created. Skipping this step.\n",
      "Extra evaluation metadata files have already been created. Skipping this step.\n",
      "Vocabulary file already exists. Skipping this step.\n",
      "Calling setup!\n",
      "Training using matched utterances!\n",
      "vocab_size = 2350\n",
      "load model from checkpoints/lm_dataset_saycam_captioning_False_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_1/epoch=38.ckpt\n",
      "load model from checkpoints/lm_dataset_saycam_captioning_False_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_2/epoch=28.ckpt\n",
      "load model from checkpoints/lm_dataset_saycam_captioning_True_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_0/epoch=29.ckpt\n",
      "load model from checkpoints/lm_dataset_saycam_captioning_True_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_1/epoch=42.ckpt\n",
      "load model from checkpoints/lm_dataset_saycam_captioning_True_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_2/epoch=38.ckpt\n",
      "load model from checkpoints/lm_dataset_saycam_text_encoder_cbow_embedding_dim_512_tie_False_bias_False_crange_1_dropout_i_0.0_dropout_o_0.1_batch_size_8_lr_0.003_lr_scheduler_True_patience_2_weight_decay_0.04_seed_0/epoch=31.ckpt\n",
      "load model from checkpoints/lm_dataset_saycam_text_encoder_cbow_embedding_dim_512_tie_False_bias_False_crange_1_dropout_i_0.0_dropout_o_0.1_batch_size_8_lr_0.003_lr_scheduler_True_patience_2_weight_decay_0.04_seed_1/epoch=65.ckpt\n",
      "load model from checkpoints/lm_dataset_saycam_text_encoder_cbow_embedding_dim_512_tie_False_bias_False_crange_1_dropout_i_0.0_dropout_o_0.1_batch_size_8_lr_0.003_lr_scheduler_True_patience_2_weight_decay_0.04_seed_2/epoch=58.ckpt\n"
     ]
    }
   ],
   "source": [
    "# load model from checkpoint\n",
    "\n",
    "# select from list of checkpoints\n",
    "dataset_name = \"saycam\"\n",
    "ori_names = {\n",
    "    \"saycam\": [\n",
    "        \"LSTM 0\", \"LSTM 1\", \"LSTM 2\",\n",
    "        \"LSTM Captioning 0\", \"LSTM Captioning 1\", \"LSTM Captioning 2\",\n",
    "        \"CBOW 0\", \"CBOW 1\", \"CBOW 2\",\n",
    "    ],\n",
    "    \"coco\": [\n",
    "        \"lm\",\n",
    "        \"capt_ft\",\n",
    "        \"capt_attn_gt_ft\",\n",
    "        \"capt_attn_gt_reg_ft\",\n",
    "        \"capt_attn_gt_reg_untie_ft\",\n",
    "        \"cbow\",\n",
    "    ],\n",
    "}[dataset_name]\n",
    "dataset_checkpoint_paths = all_checkpoint_paths[dataset_name]\n",
    "ori_checkpoint_paths = [dataset_checkpoint_paths[ori_name] for ori_name in ori_names]\n",
    "\n",
    "ori_models = []\n",
    "\n",
    "data = None\n",
    "\n",
    "for checkpoint_path in ori_checkpoint_paths:\n",
    "    if \"gram\" in checkpoint_path:\n",
    "        ngram_model = build_ngram_model(int(checkpoint_path.split('-')[0]), vocab_size, data.train_dataloader())\n",
    "        model = ngram_model\n",
    "\n",
    "    else:\n",
    "        print_dict_args = False\n",
    "        if print_dict_args:\n",
    "            ckpt = torch.load(checkpoint)\n",
    "            print(ckpt['hyper_parameters']['args'])\n",
    "\n",
    "        print(f\"load model from {checkpoint_path}\")\n",
    "        lit_model = MultiModalLitModel.load_from_checkpoint(checkpoint_path, map_location=device)\n",
    "        #print(lit_model.args)\n",
    "        lit_model.to(device)\n",
    "\n",
    "        if data is None:\n",
    "            # build data and vocab according to the model\n",
    "            data, args = build_data(args=lit_model.args, return_args=True)\n",
    "            dataset_name = args.dataset\n",
    "\n",
    "            word2idx = lit_model.text_encoder.word2idx\n",
    "            idx2word = lit_model.text_encoder.idx2word\n",
    "\n",
    "            vocab = lit_model.text_encoder.vocab\n",
    "            vocab_size = len(vocab)\n",
    "            print(f'vocab_size = {vocab_size}')\n",
    "            # check consistency between vocab and idx2word\n",
    "            for idx in range(vocab_size):\n",
    "                assert idx in idx2word\n",
    "\n",
    "        else:\n",
    "            assert lit_model.args[\"dataset\"] == dataset_name, f\"checkpoint {checkpoint_path} ran on a different dataset {args.dataset}\"\n",
    "\n",
    "        lit_model.eval()\n",
    "        model = lit_model\n",
    "\n",
    "    ori_models.append(model)\n",
    "\n",
    "\n",
    "# each name represents a group of models that we want to use their mean predictions;\n",
    "# for example, if 'lm' is in this list, then predictions of all model with name 'lm*' is aggregated into 'lm'\n",
    "names = ['LSTM', 'LSTM Captioning', 'CBOW', 'Contrastive', 'Joint bs16', 'Joint bs512'][:-3] if True else ori_names\n",
    "\n",
    "groups = {name: [] for name in names}\n",
    "\n",
    "for i, ori_name in enumerate(ori_names):\n",
    "    best_name = ''\n",
    "    for name in names:\n",
    "        if ori_name.startswith(name) and len(name) > len(best_name):\n",
    "            best_name = name\n",
    "    groups[best_name].append(i)\n",
    "\n",
    "models = []\n",
    "for name, group in groups.items():\n",
    "    assert group, f\"no models corresponds to {name}\"\n",
    "    models.append(ori_models[group[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached pos tags: dataset_cache/saycam/train.pos.cache\n",
      "load from checkpoints/lm_dataset_saycam_captioning_False_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_0/epoch=29.ckpt.train.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_captioning_False_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_1/epoch=38.ckpt.train.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_captioning_False_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_2/epoch=28.ckpt.train.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_captioning_True_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_0/epoch=29.ckpt.train.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_captioning_True_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_1/epoch=42.ckpt.train.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_captioning_True_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_2/epoch=38.ckpt.train.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_text_encoder_cbow_embedding_dim_512_tie_False_bias_False_crange_1_dropout_i_0.0_dropout_o_0.1_batch_size_8_lr_0.003_lr_scheduler_True_patience_2_weight_decay_0.04_seed_0/epoch=31.ckpt.train.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_text_encoder_cbow_embedding_dim_512_tie_False_bias_False_crange_1_dropout_i_0.0_dropout_o_0.1_batch_size_8_lr_0.003_lr_scheduler_True_patience_2_weight_decay_0.04_seed_1/epoch=65.ckpt.train.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_text_encoder_cbow_embedding_dim_512_tie_False_bias_False_crange_1_dropout_i_0.0_dropout_o_0.1_batch_size_8_lr_0.003_lr_scheduler_True_patience_2_weight_decay_0.04_seed_2/epoch=58.ckpt.train.cache.pt\n",
      "NN   <unk>        4364:    27.586    24.645    37.825\n",
      ".    <sos>       33737:     1.000     1.000     9.869\n",
      ".    <eos>       33370:     1.462     1.409     2.862\n",
      ".    .           16566:     3.050     2.802     1.817\n",
      ",    ,            9479:     5.927     5.832     6.801\n",
      "PRP  you          9235:     5.436     5.131     3.463\n",
      ".    ?            8420:     2.382     2.105     3.315\n",
      "DT   the          5841:     5.862     5.071     3.255\n",
      "UH   yeah         4923:     8.659     8.512     4.786\n",
      "VBZ  's           4906:     2.017     1.962     3.023\n",
      "DT   a            4539:     6.554     5.162     3.592\n",
      "TO   to           4540:     2.349     2.202     2.492\n",
      "PRP  it           4435:     9.466     8.726     8.141\n",
      "CC   and          3813:    10.125     8.347     9.804\n",
      "DT   that         3742:    17.595    15.068     9.711\n",
      "PRP  we           3273:    12.192    10.776     5.585\n",
      "PRP  i            3032:    19.200    18.311     4.120\n",
      "EX   there        2921:    18.062    14.930    12.888\n",
      "VBZ  is           2862:     7.669     6.598    10.280\n",
      "VBP  do           2850:    17.917    16.139     8.508\n",
      "\n",
      "ppl             288041:     7.469     6.187    11.674\n",
      "ppl_wo_sos      254304:     9.752     7.879    11.937\n",
      "ppl_wo_sos_eos  220934:    12.990    10.218    14.810\n",
      "load cached pos tags: dataset_cache/saycam/val.pos.cache\n",
      "load from checkpoints/lm_dataset_saycam_captioning_False_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_0/epoch=29.ckpt.val.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_captioning_False_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_1/epoch=38.ckpt.val.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_captioning_False_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_2/epoch=28.ckpt.val.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_captioning_True_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_0/epoch=29.ckpt.val.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_captioning_True_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_1/epoch=42.ckpt.val.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_captioning_True_text_encoder_lstm_embedding_dim_512_dropout_i_0.5_dropout_o_0.0_batch_size_16_lr_0.006_lr_scheduler_True_weight_decay_0.04_seed_2/epoch=38.ckpt.val.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_text_encoder_cbow_embedding_dim_512_tie_False_bias_False_crange_1_dropout_i_0.0_dropout_o_0.1_batch_size_8_lr_0.003_lr_scheduler_True_patience_2_weight_decay_0.04_seed_0/epoch=31.ckpt.val.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_text_encoder_cbow_embedding_dim_512_tie_False_bias_False_crange_1_dropout_i_0.0_dropout_o_0.1_batch_size_8_lr_0.003_lr_scheduler_True_patience_2_weight_decay_0.04_seed_1/epoch=65.ckpt.val.cache.pt\n",
      "load from checkpoints/lm_dataset_saycam_text_encoder_cbow_embedding_dim_512_tie_False_bias_False_crange_1_dropout_i_0.0_dropout_o_0.1_batch_size_8_lr_0.003_lr_scheduler_True_patience_2_weight_decay_0.04_seed_2/epoch=58.ckpt.val.cache.pt\n",
      "failed to extend items by concreteness data\n",
      "failed to extend items by norm data\n",
      "NN   <unk>         295:    35.634    34.953    38.935\n",
      ".    <sos>        1874:     1.000     1.000     9.994\n",
      ".    <eos>        1857:     1.556     1.526     3.042\n",
      ".    .             941:     3.684     3.575     1.886\n",
      ",    ,             543:     7.732     8.272     8.293\n",
      "PRP  you           512:     5.923     5.827     3.533\n",
      ".    ?             452:     2.990     2.758     3.444\n",
      "DT   the           311:     8.950     8.374     4.620\n",
      "UH   yeah          251:     9.072     9.071     4.668\n",
      "VBZ  's            265:     2.122     2.122     3.205\n",
      "DT   a             238:     7.951     6.829     3.719\n",
      "TO   to            220:     3.882     3.904     3.425\n",
      "PRP  it            242:    11.236    10.751     9.312\n",
      "CC   and           220:    16.097    15.848    14.038\n",
      "DT   that          226:    24.554    22.002    10.493\n",
      "PRP  we            190:    16.151    14.660     5.782\n",
      "PRP  i             162:    21.951    21.078     4.043\n",
      "EX   there         167:    24.307    20.305    13.323\n",
      "VBZ  is            144:    11.190    11.127    13.316\n",
      "VBP  do            153:    21.250    20.322    10.201\n",
      "\n",
      "ppl              15845:    12.265    11.203    16.000\n",
      "ppl_wo_sos       13971:    17.167    15.491    17.043\n",
      "ppl_wo_sos_eos   12114:    24.804    22.098    22.195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:04<00:00,  1.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:04<00:00,  1.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:04<00:00,  1.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# get sum values (counts, vector representations, losses) across the training set\n",
    "\n",
    "split_items = {}\n",
    "\n",
    "used_splits = ['train', 'val']\n",
    "\n",
    "\n",
    "def print_items(items, n=20):\n",
    "    for index, row in items.iloc[:n].iterrows():\n",
    "        print(row_str(row, names))\n",
    "\n",
    "    print()\n",
    "\n",
    "    columns = ['cnt'] + names\n",
    "    tot_values = items.loc[items.index.map(lambda index: index[0] != PAD_TOKEN_ID), columns].sum(axis=0)\n",
    "    tot = pd.Series(tot_values, index=items.columns)\n",
    "    tot[token_field] = 'ppl'\n",
    "    print(row_str(tot, names))\n",
    "    tot_values -= items.loc[(SOS_TOKEN_ID,), columns].sum(axis=0)\n",
    "    tot = pd.Series(tot_values, index=items.columns)\n",
    "    tot[token_field] = 'ppl_wo_sos'\n",
    "    print(row_str(tot, names))\n",
    "    tot_values -= items.loc[(EOS_TOKEN_ID,), columns].sum(axis=0)\n",
    "    tot = pd.Series(tot_values, index=items.columns)\n",
    "    tot[token_field] = 'ppl_wo_sos_eos'\n",
    "    print(row_str(tot, names))\n",
    "\n",
    "\n",
    "def remove_foils_wrapper(dataloader):\n",
    "    for x, y, y_len, raw_y in dataloader:\n",
    "        yield x[:, 0], y, y_len, raw_y\n",
    "\n",
    "my_batch_size = 256\n",
    "dataloader_fns = {\n",
    "    'train': lambda: data.train_dataloader(batch_size=my_batch_size, shuffle=False, drop_last=False),\n",
    "    'val': lambda: data.val_dataloader(batch_size=my_batch_size)[0],\n",
    "    'test': lambda: data.test_dataloader(batch_size=my_batch_size)[0],\n",
    "    'eval_val': lambda: remove_foils_wrapper(data.val_dataloader()[1]),\n",
    "    'eval_test': lambda: remove_foils_wrapper(data.test_dataloader()[1]),\n",
    "}\n",
    "\n",
    "for split in used_splits:\n",
    "    dataloader_fn = dataloader_fns[split]\n",
    "\n",
    "    pos_tags = get_pos_tags(dataloader_fn(), dataset_name, split)\n",
    "\n",
    "    ori_model_items = [\n",
    "        torch_cache(checkpoint_path + f'.{split}.cache.pt')(get_model_items)(\n",
    "            model, dataloader_fn(), pos_tags, ignore_all_token_items=(split == 'train'))\n",
    "        for model, checkpoint_path in zip(ori_models, ori_checkpoint_paths)]\n",
    "    model_items = [mean_model_items([ori_model_items[i] for i in group], idx=-1) for group in groups.values()]\n",
    "    items = ModelItems(*[stack_items(items_list, names, idx2word) for items_list in list(zip(*model_items))])\n",
    "    extend_items(items.token_items, names, idx2word)\n",
    "    extend_items(items.token_pos_items, names, idx2word)\n",
    "    if items.all_token_items is not None:\n",
    "        extend_items(items.all_token_items, names, idx2word)\n",
    "    split_items[split] = items\n",
    "\n",
    "    print_items(items.token_items)\n",
    "\n",
    "    if split_items[split].all_token_items is not None:\n",
    "        for name, group in groups.items():\n",
    "            ori_probs = []\n",
    "            for i in group:\n",
    "                model = ori_models[i]\n",
    "                if isinstance(model, NGramModel):\n",
    "                    continue\n",
    "                probs = get_model_probs(model, dataloader_fns[split](), pos_tags)\n",
    "                ori_probs.append(probs)\n",
    "            probs = mean_probs(ori_probs)\n",
    "            split_items[split].all_token_items[f'{name} probs'] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cached pos tags: dataset_cache/saycam/train.pos.cache\n",
      "load cached pos tags: dataset_cache/saycam/val.pos.cache\n",
      "load cached pos tags: dataset_cache/saycam/test.pos.cache\n",
      "train #examples: 33737\n",
      "val   #examples:  1874\n",
      "test  #examples:  1875\n",
      "train #tokens: 292475\n",
      "val   #tokens:  16103\n",
      "test  #tokens:  16168\n",
      "train .               #tokens: 122736  41.96%\n",
      "train adjective       #tokens:   8190   2.80%\n",
      "train adverb          #tokens:  13679   4.68%\n",
      "train cardinal number #tokens:   1247   0.43%\n",
      "train function word   #tokens:  75610  25.85%\n",
      "train noun            #tokens:  27132   9.28%\n",
      "train verb            #tokens:  43881  15.00%\n",
      "val   .               #tokens:   6882  42.74%\n",
      "val   adjective       #tokens:    419   2.60%\n",
      "val   adverb          #tokens:    757   4.70%\n",
      "val   cardinal number #tokens:     81   0.50%\n",
      "val   function word   #tokens:   4117  25.57%\n",
      "val   noun            #tokens:   1446   8.98%\n",
      "val   verb            #tokens:   2401  14.91%\n",
      "test  .               #tokens:   6733  41.64%\n",
      "test  adjective       #tokens:    479   2.96%\n",
      "test  adverb          #tokens:    807   4.99%\n",
      "test  cardinal number #tokens:     77   0.48%\n",
      "test  function word   #tokens:   4156  25.71%\n",
      "test  noun            #tokens:   1502   9.29%\n",
      "test  verb            #tokens:   2414  14.93%\n",
      "saving plot dataset distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x2100 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot statistics of syntactic categories\n",
    "\n",
    "pos_tag_dfs = []\n",
    "\n",
    "splits = ['train', 'val', 'test']\n",
    "\n",
    "for split in splits:\n",
    "    dataloader_fn = dataloader_fns[split]\n",
    "    pos_tags = get_pos_tags(dataloader_fn(), dataset_name, split)\n",
    "\n",
    "    pos_tag_df = pd.DataFrame(data=list(itertools.chain.from_iterable(pos_tags)), columns=['pos'])\n",
    "    pos_tag_df['split'] = split\n",
    "    pos_tag_dfs.append(pos_tag_df)\n",
    "\n",
    "pos_tag_df = pd.concat(pos_tag_dfs)\n",
    "\n",
    "for pos_field, pos_mapping in pos_mappings.items():\n",
    "    pos_tag_df[pos_field] = pos_tag_df['pos'].map(pos_mapping).astype('category')\n",
    "\n",
    "pos_field = 'syntactic category'\n",
    "\n",
    "g = sns.catplot(kind='count', data=pos_tag_df, x='split', hue=pos_field, palette=pos_palette)\n",
    "g.figure.set_size_inches(*figsize)\n",
    "\n",
    "for split in splits:\n",
    "    print(f'{split:5s} #examples: {len(data.datasets[split]):5d}')\n",
    "\n",
    "for split in splits:\n",
    "    split_cnt = len(pos_tag_df[(pos_tag_df['split'] == split)])\n",
    "    print(f'{split:5s} #tokens: {split_cnt:6d}')\n",
    "\n",
    "for (split, pos), bar in zip(itertools.product(splits, pos_tag_df.dtypes[pos_field].categories), g.ax.patches):\n",
    "    split_cnt = len(pos_tag_df[(pos_tag_df['split'] == split)])\n",
    "    split_pos_cnt = len(pos_tag_df[(pos_tag_df['split'] == split) & (pos_tag_df[pos_field] == pos)])\n",
    "    print(f'{split:5s} {pos:15s} #tokens: {split_pos_cnt:6d} {split_pos_cnt / split_cnt :7.2%}')\n",
    "    g.ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 40, f'{split_pos_cnt:d}', ha=\"center\", fontsize='xx-small')\n",
    "\n",
    "output_fig('dataset distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example #101:\n",
      "normalizing: [vmin, vmax] = [-0.005486, 0.068414] to [0, 1]\n",
      "saving plot example101 visual map\n",
      "you love that ball .\n",
      "saving plot example101 loss heatmap\n",
      "LSTM:\n",
      "  2.0% ball     |  19.4% ,         14.8% .          9.6% one        8.4% book       5.8% ?       \n",
      "\n",
      "LSTM Captioning:\n",
      " 14.4% ball     |  16.9% .         14.4% ball       8.1% one        7.4% ,          6.4% ?       \n",
      "\n",
      "CBOW:\n",
      "  0.7% ball     |  17.8% <eos>      8.6% one        6.3% is         5.3% <unk>      3.2% way     \n",
      "\n",
      "example #173:\n",
      "normalizing: [vmin, vmax] = [-0.008447, 0.131133] to [0, 1]\n",
      "saving plot example173 visual map\n",
      "there s a ball and puppies and bunnies and bears and carrots .\n",
      "saving plot example173 loss heatmap\n",
      "LSTM:\n",
      " 10.6% ball     |  10.6% ball       8.3% kitty      7.8% baby       6.0% car        4.4% doggy   \n",
      "\n",
      "LSTM Captioning:\n",
      "  5.7% ball     |   9.7% kitty      6.9% car        5.7% ball       4.9% frog       3.6% doggy   \n",
      "\n",
      "CBOW:\n",
      "  1.7% ball     |   7.2% <sos>      4.4% <unk>      3.7% kitty      3.2% heres      2.4% baby    \n",
      "\n",
      "example #215:\n",
      "normalizing: [vmin, vmax] = [-0.003351, 0.042456] to [0, 1]\n",
      "saving plot example215 visual map\n",
      "that 's a ball , yeah .\n",
      "saving plot example215 loss heatmap\n",
      "LSTM:\n",
      "  2.2% ball     |  12.0% big        5.5% <unk>      5.4% good       4.7% kitty      4.5% train   \n",
      "\n",
      "LSTM Captioning:\n",
      "  0.8% ball     |  12.7% kitty      9.3% baby       5.0% puppy      4.3% big        3.7% chick   \n",
      "\n",
      "CBOW:\n",
      "  2.1% ball     |   5.7% and        4.1% kitty      3.2% doggy      3.0% <unk>      2.3% baby    \n",
      "\n",
      "example #257:\n",
      "normalizing: [vmin, vmax] = [-0.006331, 0.057856] to [0, 1]\n",
      "saving plot example257 visual map\n",
      "a ball .\n",
      "saving plot example257 loss heatmap\n",
      "LSTM:\n",
      "  2.3% ball     |   7.1% little     4.8% flower     3.9% big        3.8% baby       2.9% kitty   \n",
      "\n",
      "LSTM Captioning:\n",
      " 19.6% ball     |  19.6% ball       9.5% little     3.7% good       3.6% big        3.4% yellow  \n",
      "\n",
      "CBOW:\n",
      "  3.8% ball     |   5.3% <unk>      3.9% kitty      3.8% ball       2.7% banana     2.3% lot     \n",
      "\n",
      "example #353:\n",
      "normalizing: [vmin, vmax] = [-0.002999, 0.035862] to [0, 1]\n",
      "saving plot example353 visual map\n",
      "is that one maybe the ball ?\n",
      "saving plot example353 loss heatmap\n",
      "LSTM:\n",
      "  7.6% ball     |   7.6% ball       5.3% other      4.2% beach      3.1% problem    2.9% <unk>   \n",
      "\n",
      "LSTM Captioning:\n",
      " 48.2% ball     |  48.2% ball       5.2% one        3.8% small      3.8% other      3.7% cup     \n",
      "\n",
      "CBOW:\n",
      "  9.5% ball     |   9.5% ball       3.7% in         2.6% book       2.4% train      2.3% <unk>   \n",
      "\n",
      "normalizing: [vmin, vmax] = [-0.005486, 0.068414] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.006331, 0.057856] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.002999, 0.035862] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.004230, 0.082732] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.002885, 0.023151] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.004910, 0.035831] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.002749, 0.017069] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.004185, 0.045989] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.004020, 0.044995] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.004394, 0.030048] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.016521, 0.190466] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.004292, 0.041836] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.004636, 0.040853] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.002719, 0.036236] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.016278, 0.188431] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.008782, 0.082568] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.002870, 0.050245] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.004973, 0.072927] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.004177, 0.038719] to [0, 1]\n",
      "normalizing: [vmin, vmax] = [-0.011639, 0.092532] to [0, 1]\n",
      "saving plot LSTM Captioning GradCAM Examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1680x367.5 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3360x367.5 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2100x367.5 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1260x367.5 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2100x367.5 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3300x2640 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "searched_word = \"ball\"\n",
    "visualize_models = True\n",
    "n_print_example = 5\n",
    "textgen_model_ns = [model_n for model_n, (name, model) in enumerate(zip(names, models)) if 'attn' in name] if False else []\n",
    "multiple_views = False\n",
    "all_steps = False\n",
    "\n",
    "gradcam_model_ns = [model_n for model_n, model in enumerate(models) if model.text_encoder.captioning or model.text_encoder.has_attention]\n",
    "attn_model_ns = [model_n for model_n, model in enumerate(models) if model.text_encoder.has_attention]\n",
    "if not visualize_models:\n",
    "    gradcam_model_ns = []\n",
    "    attn_model_ns = []\n",
    "n_visualized_models = len(gradcam_model_ns) + len(attn_model_ns)\n",
    "\n",
    "if searched_word:\n",
    "    if searched_word is True:\n",
    "        searched_word = input(\"search word: \")\n",
    "    searched_token_id = word2idx.get(searched_word, UNK_TOKEN_ID)\n",
    "    if searched_token_id == UNK_TOKEN_ID:\n",
    "        print(f\"mapping {searched_word} to UNK\")\n",
    "\n",
    "\n",
    "def get_views(x, grid=(2, 2)):\n",
    "    \"\"\"Get multiple views of image x.\n",
    "    \"\"\"\n",
    "    img = n_inv(x)\n",
    "    views = [img]\n",
    "    h = img.size(-2) // grid[0]\n",
    "    w = img.size(-1) // grid[1]\n",
    "    for grid_i in range(grid[0]):\n",
    "        for grid_j in range(grid[1]):\n",
    "            x = grid_i * h\n",
    "            y = grid_j * w\n",
    "            views.append(resized_crop(img, x, y, h, w, img.shape[-2:]))\n",
    "    views = list(map(normalizer, views))\n",
    "    return views\n",
    "\n",
    "\n",
    "show_image = functools.partial(\n",
    "    plot_image,\n",
    "    overlying=True,\n",
    "    blur=False,\n",
    "    #interpolation='nearest',\n",
    ")\n",
    "\n",
    "\n",
    "def visualize_example(x, y, y_len, raw_y, steps=None, model_first=True, prepend_x=False, use_losses=None, example_name='example'):\n",
    "    \"\"\"Visualize an example.\n",
    "    Inputs:\n",
    "        steps: list of steps to visualize; None for all steps\n",
    "        model_first: if True, then the axes are of n_visualized_models * len(steps), else it is transposed\n",
    "        prepend_x: if True, prepend raw image x before the models\n",
    "        use_losses: use the designated losses; must be a list of losses, where each losses is a list of loss at each step;\n",
    "            for example, split_items[split].losses[example_i]; if None, use the losses generated by running the models\n",
    "    \"\"\"\n",
    "    img = torch_to_numpy_image(n_inv(x))\n",
    "    y_labels = [idx2word[y_id.item()] for y_id in y]\n",
    "    if steps is None:\n",
    "        steps = list(range(y_len.item()))\n",
    "\n",
    "    n_rows, n_cols = int(prepend_x) + n_visualized_models, len(steps)\n",
    "    if not model_first:\n",
    "        n_rows, n_cols = n_cols, n_rows\n",
    "    n_axes = n_rows * n_cols\n",
    "    ax_size = 5\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(ax_size * n_cols, ax_size * n_rows), squeeze=False)\n",
    "    if not model_first:\n",
    "        axes = [[axes[j][i] for j in range(n_rows)] for i in range(n_cols)]\n",
    "    axes_iter = itertools.chain.from_iterable(axes)\n",
    "\n",
    "    if prepend_x:\n",
    "        for step_i, step in enumerate(steps):\n",
    "            show_image(next(axes_iter), img)\n",
    "\n",
    "    for model_n in gradcam_model_ns:\n",
    "        model, name = models[model_n], names[model_n]\n",
    "        gradcams = gradCAM_for_captioning_lm(model, x, y, y_len, steps=steps)\n",
    "        prefix = f'{name} GradCAM'\n",
    "        for step_i, (step, gradcam) in enumerate(zip(steps, gradcams)):\n",
    "            if step == 0:\n",
    "                show_image(next(axes_iter), img, text=prefix)\n",
    "            else:\n",
    "                text = y_labels[step]\n",
    "                if step_i == 0:\n",
    "                    text = prefix + ' ' + text\n",
    "                show_image(next(axes_iter), img, gradcam, text=text)\n",
    "\n",
    "    for model_n in attn_model_ns:\n",
    "        model, name = models[model_n], names[model_n]\n",
    "        attns = attention_for_attention_lm(model, x, y, y_len, steps=steps)\n",
    "        prefix = f'{name} attn'\n",
    "        for step_i, (step, attn) in enumerate(zip(steps, attns)):\n",
    "            text = y_labels[step]\n",
    "            if step_i == 0:\n",
    "                text = prefix + ' ' + text\n",
    "            show_image(next(axes_iter), img, attn, text=text)\n",
    "\n",
    "    for ax in axes_iter:\n",
    "        ax.axis(\"off\")\n",
    "    output_fig(example_name + ' visual map')\n",
    "\n",
    "    if use_losses is None:\n",
    "        with torch.no_grad():\n",
    "            rets = [run_model(model, x, y, y_len, single_example=True, return_all=True) for model in models]\n",
    "            losses = [ret[1].cpu().numpy() for ret in rets]\n",
    "    else:\n",
    "        losses = use_losses\n",
    "    print(raw_y[0])\n",
    "    names_, losses_ = zip(*[(name, loss) for name, loss in zip(names, losses) if 'contrastive' not in name])\n",
    "    plot_model_y_value_heatmap(names_, losses_, y_labels)\n",
    "    output_fig(example_name+' loss heatmap')\n",
    "\n",
    "    if use_losses is None:\n",
    "        for name, model, ret in zip(names, models, rets):\n",
    "            if not isinstance(model, MultiModalLitModel) or 'contrastive' in name:\n",
    "                continue\n",
    "            print(f'{name}:')\n",
    "            if model.language_model.text_encoder.regressional:\n",
    "                steps_ = [step - 1 for step in steps if step > 0]\n",
    "            else:\n",
    "                steps_ = steps\n",
    "            logits, labels = ret[2], ret[4]\n",
    "            probs = logits.softmax(-1)\n",
    "            print_top_values(probs, idx2word, labels, steps=steps_, value_formatter=prob_formatter)\n",
    "            print()\n",
    "\n",
    "\n",
    "#examples = examples_from_dataloader(dataloader_fns['val']())\n",
    "\n",
    "example_i = 0\n",
    "print_example_i = 0\n",
    "\n",
    "#for example_i, (x, y, y_len, raw_y) in enumerate(examples):\n",
    "dataset = data.datasets['val']\n",
    "for example_i in range(len(dataset)):\n",
    "    x, y, y_len, raw_y = dataset[example_i]\n",
    "    y_len = torch.tensor(y_len)\n",
    "    y = y[:y_len]\n",
    "\n",
    "    if searched_word:\n",
    "        searched_word_steps = [idx for idx, y_id in enumerate(y) if y_id == searched_token_id]\n",
    "        if not searched_word_steps:\n",
    "            continue\n",
    "    \n",
    "    print(f'example #{example_i}:')\n",
    "\n",
    "    if all_steps:\n",
    "        steps = None\n",
    "    else:\n",
    "        steps = searched_word_steps if searched_word else [0]\n",
    "\n",
    "    for x_view_i, x_view in enumerate(get_views(x) if multiple_views else [x]):\n",
    "        visualize_example(x_view, y, y_len, raw_y, steps=steps, model_first=all_steps, prepend_x=not all_steps, example_name=f'example{example_i}')\n",
    "\n",
    "    for model_n in textgen_model_ns:\n",
    "        name = names[model_n]\n",
    "        print(f\"generating text from {name}:\")\n",
    "        model = models[model_n]\n",
    "        image_features, image_feature_map = model.model.encode_image(x.unsqueeze(0).to(device))\n",
    "        beam_seq, log_prob = model.language_model.beam_search_decode(\n",
    "            batch_size=1,\n",
    "            beam_width=model.beam_width,\n",
    "            decode_length=model.decode_length,\n",
    "            length_penalty_alpha=model.length_penalty_alpha,\n",
    "            image_features=image_features if model.language_model.text_encoder.captioning else None,\n",
    "            image_feature_map=image_feature_map if model.language_model.text_encoder.has_attention else None,\n",
    "        )\n",
    "        gen_text_ids = beam_seq[0, 0]\n",
    "        gen_text_len = len(gen_text_ids)\n",
    "        while gen_text_len > 0 and gen_text_ids[gen_text_len - 1] == PAD_TOKEN_ID:\n",
    "            gen_text_len -= 1\n",
    "        gen_text_len = torch.tensor(gen_text_len, device=device)\n",
    "        gen_text_labels = [idx2word[y_id.item()] for y_id in gen_text_ids]\n",
    "        gen_text = ' '.join(gen_text_labels)\n",
    "        visualize_example(x, gen_text_ids, gen_text_len, [gen_text], steps=None, model_first=True, prepend_x=False, example_name=f'example{example_i} {name}')\n",
    "\n",
    "    print_example_i += 1\n",
    "    if print_example_i >= n_print_example:\n",
    "        break\n",
    "\n",
    "\n",
    "from textwrap import wrap\n",
    "import re\n",
    "\n",
    "shown_examples = {\n",
    "    'ball': [101, 257, 353, 651, 756],\n",
    "    'kitty': [97, 285, 442, 1634, 1862],\n",
    "    'banana': [27, 657, 1137, 1419, 1687],\n",
    "    #'hand': [156, 1322, 1402, 1432, 1513],\n",
    "    #'sand': [307, 1712, 1763],\n",
    "    'baby': [256, 495, 1072, 1139, 1497],\n",
    "}\n",
    "n_rows = len(shown_examples)\n",
    "n_cols = 5\n",
    "ax_size = 2.2\n",
    "for model_n in gradcam_model_ns:\n",
    "    model, name = models[model_n], names[model_n]\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(ax_size * n_cols, ax_size * n_rows), squeeze=False)\n",
    "    for (searched_word, example_ids), axes_line in zip(shown_examples.items(), axes):\n",
    "        searched_token_id = word2idx.get(searched_word, UNK_TOKEN_ID)\n",
    "        for example_i, ax in zip(example_ids, axes_line):\n",
    "            x, y, y_len, raw_y = dataset[example_i]\n",
    "            y_len = torch.tensor(y_len)\n",
    "            utterance = raw_y[0]\n",
    "            img = torch_to_numpy_image(n_inv(x))\n",
    "            searched_word_steps = [idx for idx, y_id in enumerate(y) if y_id == searched_token_id]\n",
    "            assert len(searched_word_steps) == 1\n",
    "            gradcams = gradCAM_for_captioning_lm(model, x, y, y_len, steps=searched_word_steps)\n",
    "            step = searched_word_steps[0]\n",
    "            gradcam = gradcams[0]\n",
    "            show_image(ax, img, gradcam)\n",
    "            wrapped_utterance = '\\n'.join(wrap(utterance, 40))\n",
    "            wrapped_utterance = re.sub(r'\\b'+searched_word+r'\\b', r'$\\\\bf{'+searched_word+r'}$', wrapped_utterance)\n",
    "            ax.text(0.5, -0.05, wrapped_utterance, ha='center', va='top', transform=ax.transAxes, fontsize='xx-small')\n",
    "        ax = axes_line[0]\n",
    "        ax.text(-0.3, 0.5, searched_word, ha='center', va='top', transform=ax.transAxes, fontsize='medium')\n",
    "    output_fig(f'{name} GradCAM Examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_attr, repres_name = [('mean_vector', 'Mean Representation'), ('embedding', 'Embedding')][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine matrices:\n",
      "\n",
      "saving plot color\n",
      "saving plot people\n",
      "cosine matrices for the differentiations:\n",
      "\n",
      "saving plot VB-VBZ\n",
      "saving plot VB-VBD\n",
      "saving plot VB-VBG\n",
      "saving plot male-female\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 6930x4620 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 5040x3360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 10080x3360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 12600x4200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 15120x5040 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 5040x1680 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cosine matrices for some tested words\n",
    "\n",
    "def split_tokens(inp):\n",
    "    tokens = inp.split()\n",
    "    token_ids = []\n",
    "    for token in tokens:\n",
    "        token_id = word2idx.get(token, UNK_TOKEN_ID)\n",
    "        if token_id == UNK_TOKEN_ID:\n",
    "            print(f'mapping {token} to UNK')\n",
    "        token_ids.append(token_id)\n",
    "    return token_ids\n",
    "\n",
    "def get_items_from_inp(inp, token_items=split_items['train'].token_items):\n",
    "    token_ids = split_tokens(inp)\n",
    "    if UNK_TOKEN_ID in token_ids:\n",
    "        raise KeyError\n",
    "    return token_items.loc[token_ids]\n",
    "\n",
    "\n",
    "# cosine matrices\n",
    "print('cosine matrices:')\n",
    "print()\n",
    "for figname, inp in {\n",
    "    \"color\": \"red orange yellow green blue purple brown black white\",\n",
    "    \"people\": \"boy girl mommy daddy grandpa grandma\",\n",
    "}.items():\n",
    "    try:\n",
    "        items = get_items_from_inp(inp)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    plot_vector_sim_heatmap(items, names, vector_attr=vector_attr, one_figure=True, figname=figname)\n",
    "\n",
    "# cosine matrices for the differentiations (vector1 - vector0)\n",
    "print('cosine matrices for the differentiations:')\n",
    "print()\n",
    "for figname, inp in {\n",
    "    \"VB-VBZ\": \"do does go goes play plays get gets have has make makes look looks\",\n",
    "    \"VB-VBD\": \"do did go went play played get got eat ate have had make made look looked fly flew\",\n",
    "    \"VB-VBG\": \"do doing go going play playing get getting eat eating have having look looking fly flying drive driving stand standing crawl crawling\",\n",
    "    \"male-female\": \"boy girl mommy daddy grandpa grandma\",\n",
    "}.items():\n",
    "    try:\n",
    "        items = get_items_from_inp(inp)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    plot_vector_sim_heatmap(items, names, diff=True, vector_attr=vector_attr, one_figure=True, figname=figname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-SNE done.\n",
      "SVD done.\n",
      "PCA done.\n",
      "LSTM Captioning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-SNE done.\n",
      "SVD done.\n",
      "PCA done.\n",
      "CBOW:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-SNE done.\n",
      "SVD done.\n",
      "PCA done.\n",
      "LSTM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-SNE done.\n",
      "SVD done.\n",
      "PCA done.\n",
      "LSTM Captioning:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-SNE done.\n",
      "SVD done.\n",
      "PCA done.\n",
      "CBOW:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "/home/ww2135/.local/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:819: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-SNE done.\n",
      "SVD done.\n",
      "PCA done.\n"
     ]
    }
   ],
   "source": [
    "for split in used_splits:\n",
    "    token_items = split_items[split].token_items\n",
    "\n",
    "    for n, name in enumerate(names):\n",
    "        print(f'{name}:')\n",
    "        get_tsne_points(token_items[name], get_attr=vector_attr)\n",
    "        extend_point_items(token_items, name, 'tsne')\n",
    "        get_eigen_points(token_items[name], get_attr=vector_attr)\n",
    "        extend_point_items(token_items, name, 'eigen')\n",
    "        get_pca_points(token_items[name], get_attr=vector_attr)\n",
    "        extend_point_items(token_items, name, 'pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split:\n",
      "number of .s: 222\n",
      ".    <sos>       33737:     1.000     1.000     9.869\n",
      ".    <eos>       33370:     1.462     1.409     2.862\n",
      ".    .           16566:     3.050     2.802     1.817\n",
      ",    ,            9479:     5.927     5.832     6.801\n",
      ".    ?            8420:     2.382     2.105     3.315\n",
      "UH   yeah         4923:     8.659     8.512     4.786\n",
      "``   \"            2043:    13.632     5.386    20.110\n",
      ".    !            1819:    12.186     9.430    12.480\n",
      "UH   okay         1760:    26.081    25.654    18.520\n",
      "UH   oh           1272:    30.678    31.567    16.537\n",
      "UH   ok            812:    62.735    53.224    52.453\n",
      ",    ...           748:    45.401    35.485   125.832\n",
      "UH   no            668:    67.770    52.507    50.516\n",
      "UH   alright       642:    75.060    69.066    38.375\n",
      ",    -             486:    18.118    13.404    23.628\n",
      "UH   huh           253:    13.403    10.324    28.507\n",
      "UH   well          250:    82.771    70.561   112.284\n",
      "UH   yea           244:   149.527    51.817    55.726\n",
      ".    ..            234:   156.156   108.171   296.951\n",
      "UH   uh            210:   207.556   156.190    14.485\n",
      "UH   hi            206:   133.266    72.655    82.237\n",
      ",    --            178:   235.260    78.211   486.529\n",
      "UH   yeahh         168:   216.326   105.508   117.842\n",
      "NFP  _             159:     4.457     4.231     1.867\n",
      ",    …             144:   301.919   109.032   767.167\n",
      "UH   ah            120:   337.206   246.544   157.232\n",
      "''   '             108:    41.416    11.780   237.214\n",
      "UH   hey           102:   336.488   260.992   106.447\n",
      "UH   yes            97:   364.101   265.948   169.126\n",
      "UH   yep            95:   348.237   298.332   186.710\n",
      "UH   hello          94:   251.095   172.872   196.867\n",
      "UH   mm             90:   498.108   275.308   158.884\n",
      "UH   nope           80:   237.812   206.752   247.810\n",
      ":    –              80:   353.829    61.049   581.486\n",
      ".    ....           79:    75.594    47.119   294.010\n",
      "UH   wow            77:   491.563   304.926   381.977\n",
      "UH   mmm            65:   549.461   293.207   206.633\n",
      "UH   hmm            64:   350.668   258.672   387.430\n",
      "UH   yay            61:   449.926   280.758   102.615\n",
      "UH   ohh            60:   641.780   496.906   502.518\n",
      "UH   um             58:   578.933   184.198   691.674\n",
      "UH   yup            51:   683.069   523.497   278.397\n",
      "UH   ya             50:   674.810   127.939    92.713\n",
      "UH   yeahhh         49:   424.076   274.266   498.106\n",
      "UH   mmkay          44:   757.855   449.023   252.407\n",
      "UH   ahh            43:   962.322   763.979   659.021\n",
      "UH   op             42:   409.259   199.875   141.519\n",
      "UH   yum            41:    50.591    15.125    15.537\n",
      "FW   nom            40:     4.006     3.103    15.262\n",
      "UH   ooh            38:   433.040   269.110   395.838\n",
      "number of adjectives: 221\n",
      "JJ   right         858:     9.244     9.000    16.083\n",
      "JJ   little        497:    26.219    20.470     6.133\n",
      "JJR  more          495:    17.697    15.182    20.833\n",
      "JJ   good          470:    44.863    36.114    17.091\n",
      "JJ   big           318:    35.095    26.124    17.873\n",
      "JJ   other         266:    25.472    21.680     8.442\n",
      "JJ   ready         220:   116.681    84.494   127.903\n",
      "JJ   yummy         166:    81.556    33.664    53.210\n",
      "JJ   hard          150:    23.223    17.385    33.942\n",
      "JJ   else          137:     5.140     6.034    16.116\n",
      "JJ   red           137:   148.484    50.561   222.854\n",
      "JJ   lovely        134:   100.823    77.546   360.094\n",
      "JJ   nice          126:    67.252    55.475   117.278\n",
      "JJ   blue          119:   175.665    72.889   229.620\n",
      "JJ   sure          117:    41.121    36.539   105.034\n",
      "JJ   happy         112:    16.367     9.065    42.776\n",
      "JJ   hot           105:    47.617    17.996   207.641\n",
      "JJ   green         104:   148.997    73.662   238.587\n",
      "JJ   different     101:    52.007    40.434    26.133\n",
      "JJR  better        101:    56.899    56.151   131.753\n",
      "JJ   yellow         86:    81.101    39.061   291.995\n",
      "JJ   new            76:   102.495    68.929    19.722\n",
      "JJ   brown          74:   111.635    39.373   246.475\n",
      "JJ   sorry          73:    84.256    82.977    76.663\n",
      "JJ   orange         72:    34.774    12.713    99.848\n",
      "JJ   close          71:    66.764    48.232   250.866\n",
      "JJ   cold           63:    36.307    32.470   338.155\n",
      "JJ   purple         62:    93.895    29.451   417.422\n",
      "JJ   sad            61:    19.989    11.594    96.625\n",
      "JJ   weird          61:    54.544    35.667   130.648\n",
      "JJ   tired          60:    12.366     9.855   102.668\n",
      "JJ   many           50:    11.341     5.928    11.602\n",
      "JJ   wrong          49:     8.816     6.830    75.304\n",
      "JJ   white          47:   104.784    35.065   321.151\n",
      "JJ   bad            46:    30.062    23.642   193.465\n",
      "JJ   hungry         46:    14.330    11.034   242.012\n",
      "JJ   dark           46:    35.629    17.680   224.731\n",
      "JJ   enough         45:    22.132    18.711   129.593\n",
      "JJ   same           44:    13.216    10.709    24.035\n",
      "JJ   cheep          43:     2.307     1.862     2.090\n",
      "JJ   fine           42:    60.395    49.359   325.338\n",
      "JJ   black          41:   120.603    63.038   657.053\n",
      "JJ   long           40:    36.806    32.205    98.285\n",
      "JJ   last           39:    70.428    46.585    17.510\n",
      "JJ   few            38:    15.009    11.330     6.809\n",
      "JJ   fluffy         38:    39.609     9.996     7.611\n",
      "JJ   wet            37:    27.988    21.620   154.301\n",
      "JJ   funny          36:   125.388    69.855   443.448\n",
      "JJ   careful        35:    16.071    10.758    61.060\n",
      "JJ   dry            33:    43.177    23.973   849.418\n",
      "number of adverbs: 109\n",
      "RB   here         1150:    37.639    29.693    47.358\n",
      "RB   now          1068:    42.463    41.957    63.051\n",
      "RB   so            565:    71.231    57.630    68.275\n",
      "RB   just          534:    74.559    59.822    47.220\n",
      "RB   very          451:    38.139    32.771    10.154\n",
      "RB   then          432:    23.475    21.898    56.778\n",
      "RB   too           388:    36.674    28.874    64.539\n",
      "RB   again         378:    16.017    12.471    28.657\n",
      "RB   back          354:    16.445    13.812    56.782\n",
      "RB   still         191:   100.251   103.111   112.771\n",
      "RB   outside       177:    28.011    14.354    54.544\n",
      "RB   pretty        161:    65.510    55.802    33.053\n",
      "RB   away          158:     5.522     4.794    39.552\n",
      "RB   around        149:     8.824     6.550    64.718\n",
      "RB   maybe         119:   316.114   271.776   340.957\n",
      "RB   though        117:    39.074    23.038   211.479\n",
      "RB   also          117:   155.809   132.906   269.640\n",
      "RB   first         116:    76.030    68.823   227.392\n",
      "RB   really        109:   201.337   183.887   138.019\n",
      "RB   as            101:    26.651    24.152    17.753\n",
      "RB   much           94:    17.962    14.683    65.914\n",
      "RB   actually       94:   643.236   544.992   502.344\n",
      "RB   inside         80:    48.846    27.911   266.075\n",
      "RB   almost         78:   209.501   170.613   229.772\n",
      "RB   only           63:   380.725   309.681   286.891\n",
      "RB   yet            63:     9.241     6.278   152.186\n",
      "RB   home           59:    14.574     9.222    75.676\n",
      "RB   sometimes      59:   151.596    92.060   495.922\n",
      "RB   probably       58:   434.618   298.074   213.807\n",
      "RB   together       55:    26.086    13.095   147.962\n",
      "RB   next           51:   126.623    78.431    45.770\n",
      "RB   already        50:   184.358   149.713   193.149\n",
      "RB   even           49:   197.370   124.193    79.925\n",
      "RB   far            46:    26.549    18.245    43.195\n",
      "RB   soon           37:    29.368    22.356   433.153\n",
      "RB   anymore        36:    15.976    10.799   171.211\n",
      "RB   either         32:    58.968    34.941   212.773\n",
      "RB   fast           31:    16.851    14.592    61.835\n",
      "RB   later          28:    29.457    24.684   351.545\n",
      "RB   somewhere      28:   117.289    67.809   125.620\n",
      "RB   round          26:     7.911     5.614   128.792\n",
      "RB   instead        25:    27.927    20.974   294.823\n",
      "RB   never          24:    85.574    54.093    24.276\n",
      "RB   quite          24:    88.381    64.438    58.216\n",
      "RB   always         23:   254.248   142.809   565.386\n",
      "RB   ahead          22:     5.530     3.801    59.844\n",
      "RB   everywhere     21:    20.066    21.866   557.496\n",
      "RB   past           21:    37.772    18.660    66.696\n",
      "RB   kinda          20:   263.650   154.258   118.635\n",
      "RB   upside         16:    19.184     7.680    10.525\n",
      "number of cardinal numbers: 21\n",
      "CD   two           244:    45.390    36.790    41.048\n",
      "CD   three         112:    31.076    19.643   108.052\n",
      "CD   four           89:    39.132    19.861    74.617\n",
      "CD   five           42:    49.401    21.104    25.079\n",
      "CD   six            35:    35.261    16.155    90.989\n",
      "CD   eight          34:   138.549    35.001    31.831\n",
      "CD   nine           29:    79.549    17.863   151.743\n",
      "CD   seven          23:    90.960    27.083    47.969\n",
      "CD   ten            17:   106.110    21.532    29.884\n",
      "CD   2              12:    98.647    37.406  1190.021\n",
      "CD   3              11:     5.323     3.808    30.925\n",
      "CD   8              10:     7.402     2.678    52.789\n",
      "CD   1               9:   130.652    63.997  7936.858\n",
      "CD   twenty          6:  3342.788   126.406    23.991\n",
      "CD   4               6:    21.734     9.296   162.227\n",
      "CD   hundred         5:    12.374     6.201    51.510\n",
      "CD   fifty           4:  1937.539   172.374    79.539\n",
      "CD   twelve          4:     7.592     6.526  3454.431\n",
      "CD   10              3:     2.999     2.067   623.323\n",
      "CD   30              3:    43.391     3.589     1.738\n",
      "CD   5               3:   704.963   103.711   319.984\n",
      "number of function words: 137\n",
      "PRP  you          9235:     5.436     5.131     3.463\n",
      "DT   the          5841:     5.862     5.071     3.255\n",
      "TO   to           4540:     2.349     2.202     2.492\n",
      "DT   a            4539:     6.554     5.162     3.592\n",
      "PRP  it           4435:     9.466     8.726     8.141\n",
      "CC   and          3813:    10.125     8.347     9.804\n",
      "DT   that         3742:    17.595    15.068     9.711\n",
      "PRP  we           3273:    12.192    10.776     5.585\n",
      "PRP  i            3032:    19.200    18.311     4.120\n",
      "EX   there        2921:    18.062    14.930    12.888\n",
      "MD   can          1823:    15.274    13.664     6.131\n",
      "IN   on           1768:     6.420     5.590    12.285\n",
      "IN   in           1649:     7.861     7.482    17.812\n",
      "PRP$ your         1621:     9.881     9.879     4.710\n",
      "DT   this         1411:    28.523    24.724    15.477\n",
      "WP   what         1138:    36.450    31.130    27.280\n",
      "DT   all          1137:    36.251    27.725    11.401\n",
      "IN   of           1134:     2.944     2.639     3.731\n",
      "DT   some          977:    12.806    10.091     7.894\n",
      "IN   with          789:     7.792     7.257     9.056\n",
      "PRP  me            752:     5.498     4.962     7.308\n",
      "IN   at            671:     4.308     3.945     5.215\n",
      "IN   like          655:    23.599    17.988    38.478\n",
      "WRB  where         649:    69.767    43.870    29.122\n",
      "PRP  he            641:    41.759    29.136    24.280\n",
      "PRP  they          628:    39.810    31.546    24.021\n",
      "RP   out           575:    10.155     8.464    34.267\n",
      "IN   for           574:    17.099    12.590    24.097\n",
      "RP   up            568:    11.629    11.107    47.284\n",
      "CC   but           521:    56.138    42.754    63.828\n",
      "IN   if            514:    35.554    33.574    31.633\n",
      "CC   or            418:    44.755    34.240    69.591\n",
      "PRP$ my            416:    24.318    19.128     8.577\n",
      "PRP  them          406:    12.083     9.617    26.386\n",
      "MD   'll           395:    15.263    12.879     8.397\n",
      "MD   will          348:    30.039    24.522    21.260\n",
      "RP   down          348:    17.912    13.931    57.089\n",
      "RP   off           348:     7.017     6.119    51.715\n",
      "WRB  how           346:    97.645    81.639    20.551\n",
      "TO   na            339:     1.092     1.074     1.286\n",
      "MD   should        327:   114.949   109.557    29.757\n",
      "MD   wanna         285:    56.671    52.873    19.992\n",
      "IN   about         265:    12.588    10.296    18.658\n",
      "DT   an            259:    57.284    35.241     7.074\n",
      "IN   over          257:    25.949    19.075    51.851\n",
      "DT   those         251:   100.464    76.983    49.294\n",
      "DT   another       232:    62.228    49.964    27.980\n",
      "DT   these         228:    56.018    49.098    53.353\n",
      "PRP$ its           222:   146.681    79.023    52.988\n",
      "WDT  which         212:   146.329    89.342    18.946\n",
      "number of nouns: 1100\n",
      "NNP  sam           505:    48.205    36.268    93.581\n",
      "NN   ball          504:     9.229     4.456    33.163\n",
      "NN   kitty         471:    13.768     8.270    42.983\n",
      "NN   baby          390:     7.656     3.228    15.232\n",
      "NN   book          308:     5.681     4.027    56.617\n",
      "NN   train         242:    22.360    12.322    56.589\n",
      "NN   water         235:    36.356    21.865    83.275\n",
      "NN   time          228:    13.167    11.240    75.908\n",
      "NN   bear          225:    35.474    22.402    92.249\n",
      "NN   car           184:    21.185    14.405    71.850\n",
      "NN   banana        184:    27.397    10.630   111.932\n",
      "NN   poo           174:    30.004    15.905    55.634\n",
      "NN   way           173:     8.482     6.555   173.185\n",
      "NN   truck         163:    12.629     7.204    39.295\n",
      "NNS  things        160:    25.827    16.167    90.585\n",
      "NNS  shoes         152:    18.365    10.618    60.477\n",
      "NN   bunny         145:    57.011    20.520   145.401\n",
      "NN   bread         143:    32.420    13.497   158.551\n",
      "NN   doggy         142:    30.674    15.839   140.944\n",
      "NN   guy           134:     1.611     1.561     4.741\n",
      "NNS  socks         131:    23.325    12.350   135.215\n",
      "NNP  bin           126:     6.731     4.859    50.662\n",
      "NNS  eggs          125:    41.318    14.652   148.582\n",
      "NN   something     124:    89.359    69.595    88.289\n",
      "NN   nappy         124:    12.754     8.760    53.464\n",
      "NN   cup           123:    23.971    10.948   123.392\n",
      "NN   mommy         123:   107.461    57.078   227.988\n",
      "NN   job           121:     1.671     1.530     3.751\n",
      "NN   today         121:    18.860    15.193   157.879\n",
      "NN   page          118:     2.938     1.912    71.977\n",
      "NN   hand          118:     5.481     4.581    47.317\n",
      "NNS  books         117:     8.929     5.512    83.881\n",
      "NN   foot          117:     8.582     5.063    40.268\n",
      "NN   bottle        111:    14.545     8.943   119.471\n",
      "NN   egg           103:    29.958    15.023    82.044\n",
      "NN   shirt         103:    18.983    11.756   207.590\n",
      "NNS  pants          94:    28.390    20.063   116.342\n",
      "NN   potty          94:    12.985     6.839   108.645\n",
      "NN   mouth          93:     2.449     2.071    24.573\n",
      "NN   camera         93:    26.835    20.401    72.889\n",
      "NN   milk           92:    11.358     6.327    99.685\n",
      "NN   sand           89:    58.756     8.088   139.045\n",
      "NNS  hands          81:     4.566     3.783    48.500\n",
      "NNS  flowers        80:    43.480    18.126   174.675\n",
      "NNS  bubbles        77:    46.899    23.908   208.230\n",
      "NN   puzzle         76:    19.839     8.700   116.962\n",
      "NN   duck           75:    68.869    39.680   196.346\n",
      "NN   apple          73:    21.781    11.382    17.034\n",
      "NN   lid            73:    19.609     8.619    55.845\n",
      "NN   balloon        72:    50.518    19.732   224.078\n",
      "number of verbs: 496\n",
      "VBP  do           2850:    17.917    16.139     8.508\n",
      "VBP  want         2816:     4.856     4.714     4.144\n",
      "VB   go           1919:     4.699     4.309    11.635\n",
      "VBP  have         1584:    11.989    10.021    10.614\n",
      "VB   put          1299:    13.290    11.405    16.771\n",
      "VB   see          1057:    30.043    25.928    45.373\n",
      "VB   get           878:    18.489    16.741    23.311\n",
      "VB   let           875:    55.995    51.956     9.321\n",
      "VBG  going         726:     7.652     7.555     7.039\n",
      "VB   look          693:    75.564    64.503    12.668\n",
      "VBP  know          680:     7.139     7.952    14.644\n",
      "VBP  think         542:    10.290     8.151    18.461\n",
      "VB   try           472:    24.103    16.835    23.208\n",
      "VBN  done          429:     4.769     4.221    10.004\n",
      "VBD  did           410:    84.623    65.547    68.445\n",
      "VB   play          334:    30.912    22.492    19.862\n",
      "VBG  gon           332:    16.642    14.308     1.154\n",
      "VB   come          325:    91.137    71.533    26.931\n",
      "VB   read          312:    17.922     8.488    24.470\n",
      "VBD  got           293:    28.240    20.673    24.291\n",
      "VB   turn          276:    56.756    39.627    55.378\n",
      "VBP  need          275:    34.185    30.223    35.222\n",
      "VBZ  does          257:    54.387    40.573    29.095\n",
      "VB   make          235:    45.763    29.280    63.235\n",
      "VB   take          226:    60.580    59.915    60.852\n",
      "VB   remember      200:   157.081   119.365   145.187\n",
      "VB   walk          195:    29.783    16.064    69.659\n",
      "VB   give          193:    55.408    41.973    54.493\n",
      "VBZ  has           193:    17.433    11.937    33.273\n",
      "VBZ  goes          181:    13.963     9.388    31.494\n",
      "VBN  gone          175:     5.866     3.797    17.860\n",
      "VB   eat           158:    41.955    22.108   117.197\n",
      "VB   say           155:    39.926    23.863    62.909\n",
      "VB   sit           155:    80.301    70.343    44.135\n",
      "VBG  eating        119:    22.833    12.703    80.540\n",
      "VB   find          118:    40.606    28.212   102.701\n",
      "VB   hold          112:   151.977    96.251   122.191\n",
      "VBG  getting       112:    58.485    55.623    55.863\n",
      "VB   draw          109:    47.100    15.445    76.354\n",
      "VBG  doing         104:    18.208    16.235   148.054\n",
      "VBD  had           104:    36.167    29.285    67.058\n",
      "VB   help          102:    66.158    52.331    87.310\n",
      "VBP  've            98:    73.377    61.943     5.099\n",
      "VB   show           97:    16.149    10.727    22.642\n",
      "VBG  coming         96:    33.763    25.565   119.853\n",
      "VBD  went           93:    35.602    27.124    59.978\n",
      "VB   open           91:   252.878   153.138   543.059\n",
      "VB   clap           83:    11.448     6.885    91.234\n",
      "VB   pick           82:   129.525    66.585   223.709\n",
      "VB   use            81:   140.625    94.806   221.276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".    <sos>       33737:     1.000     1.000     9.869\n",
      ".    <eos>       33370:     1.462     1.409     2.862\n",
      ".    .           16566:     3.050     2.802     1.817\n",
      ",    ,            9479:     5.927     5.832     6.801\n",
      "PRP  you          9235:     5.436     5.131     3.463\n",
      ".    ?            8420:     2.382     2.105     3.315\n",
      "DT   the          5841:     5.862     5.071     3.255\n",
      "UH   yeah         4923:     8.659     8.512     4.786\n",
      "TO   to           4540:     2.349     2.202     2.492\n",
      "DT   a            4539:     6.554     5.162     3.592\n",
      "PRP  it           4435:     9.466     8.726     8.141\n",
      "CC   and          3813:    10.125     8.347     9.804\n",
      "DT   that         3742:    17.595    15.068     9.711\n",
      "PRP  we           3273:    12.192    10.776     5.585\n",
      "PRP  i            3032:    19.200    18.311     4.120\n",
      "EX   there        2921:    18.062    14.930    12.888\n",
      "VBP  do           2850:    17.917    16.139     8.508\n",
      "VBP  want         2816:     4.856     4.714     4.144\n",
      "``   \"            2043:    13.632     5.386    20.110\n",
      "VB   go           1919:     4.699     4.309    11.635\n",
      "MD   can          1823:    15.274    13.664     6.131\n",
      ".    !            1819:    12.186     9.430    12.480\n",
      "IN   on           1768:     6.420     5.590    12.285\n",
      "UH   okay         1760:    26.081    25.654    18.520\n",
      "IN   in           1649:     7.861     7.482    17.812\n",
      "PRP$ your         1621:     9.881     9.879     4.710\n",
      "VBP  have         1584:    11.989    10.021    10.614\n",
      "DT   this         1411:    28.523    24.724    15.477\n",
      "VB   put          1299:    13.290    11.405    16.771\n",
      "UH   oh           1272:    30.678    31.567    16.537\n",
      "RB   here         1150:    37.639    29.693    47.358\n",
      "WP   what         1138:    36.450    31.130    27.280\n",
      "DT   all          1137:    36.251    27.725    11.401\n",
      "IN   of           1134:     2.944     2.639     3.731\n",
      "RB   now          1068:    42.463    41.957    63.051\n",
      "VB   see          1057:    30.043    25.928    45.373\n",
      "DT   some          977:    12.806    10.091     7.894\n",
      "VB   get           878:    18.489    16.741    23.311\n",
      "VB   let           875:    55.995    51.956     9.321\n",
      "JJ   right         858:     9.244     9.000    16.083\n",
      "UH   ok            812:    62.735    53.224    52.453\n",
      "IN   with          789:     7.792     7.257     9.056\n",
      "PRP  me            752:     5.498     4.962     7.308\n",
      ",    ...           748:    45.401    35.485   125.832\n",
      "VBG  going         726:     7.652     7.555     7.039\n",
      "VB   look          693:    75.564    64.503    12.668\n",
      "VBP  know          680:     7.139     7.952    14.644\n",
      "IN   at            671:     4.308     3.945     5.215\n",
      "UH   no            668:    67.770    52.507    50.516\n",
      "IN   like          655:    23.599    17.988    38.478\n",
      "WRB  where         649:    69.767    43.870    29.122\n",
      "UH   alright       642:    75.060    69.066    38.375\n",
      "PRP  he            641:    41.759    29.136    24.280\n",
      "PRP  they          628:    39.810    31.546    24.021\n",
      "RP   out           575:    10.155     8.464    34.267\n",
      "IN   for           574:    17.099    12.590    24.097\n",
      "RP   up            568:    11.629    11.107    47.284\n",
      "RB   so            565:    71.231    57.630    68.275\n",
      "VBP  think         542:    10.290     8.151    18.461\n",
      "RB   just          534:    74.559    59.822    47.220\n",
      "CC   but           521:    56.138    42.754    63.828\n",
      "IN   if            514:    35.554    33.574    31.633\n",
      "NNP  sam           505:    48.205    36.268    93.581\n",
      "NN   ball          504:     9.229     4.456    33.163\n",
      "JJ   little        497:    26.219    20.470     6.133\n",
      "JJR  more          495:    17.697    15.182    20.833\n",
      ",    -             486:    18.118    13.404    23.628\n",
      "VB   try           472:    24.103    16.835    23.208\n",
      "NN   kitty         471:    13.768     8.270    42.983\n",
      "JJ   good          470:    44.863    36.114    17.091\n",
      "RB   very          451:    38.139    32.771    10.154\n",
      "RB   then          432:    23.475    21.898    56.778\n",
      "VBN  done          429:     4.769     4.221    10.004\n",
      "CC   or            418:    44.755    34.240    69.591\n",
      "PRP$ my            416:    24.318    19.128     8.577\n",
      "VBD  did           410:    84.623    65.547    68.445\n",
      "PRP  them          406:    12.083     9.617    26.386\n",
      "MD   'll           395:    15.263    12.879     8.397\n",
      "NN   baby          390:     7.656     3.228    15.232\n",
      "RB   too           388:    36.674    28.874    64.539\n",
      "RB   again         378:    16.017    12.471    28.657\n",
      "RB   back          354:    16.445    13.812    56.782\n",
      "MD   will          348:    30.039    24.522    21.260\n",
      "RP   down          348:    17.912    13.931    57.089\n",
      "RP   off           348:     7.017     6.119    51.715\n",
      "WRB  how           346:    97.645    81.639    20.551\n",
      "TO   na            339:     1.092     1.074     1.286\n",
      "VB   play          334:    30.912    22.492    19.862\n",
      "VBG  gon           332:    16.642    14.308     1.154\n",
      "MD   should        327:   114.949   109.557    29.757\n",
      "VB   come          325:    91.137    71.533    26.931\n",
      "JJ   big           318:    35.095    26.124    17.873\n",
      "VB   read          312:    17.922     8.488    24.470\n",
      "NN   book          308:     5.681     4.027    56.617\n",
      "VBD  got           293:    28.240    20.673    24.291\n",
      "MD   wanna         285:    56.671    52.873    19.992\n",
      "VB   turn          276:    56.756    39.627    55.378\n",
      "VBP  need          275:    34.185    30.223    35.222\n",
      "JJ   other         266:    25.472    21.680     8.442\n",
      "IN   about         265:    12.588    10.296    18.658\n",
      "\n",
      "VB   look          693:    75.564    64.503    12.668\n",
      "VBP  need          275:    34.185    30.223    35.222\n",
      "VB   draw          109:    47.100    15.445    76.354\n",
      "val split:\n",
      "number of .s: 87\n",
      ".    <sos>        1874:     1.000     1.000     9.994\n",
      ".    <eos>        1857:     1.556     1.526     3.042\n",
      ".    .             941:     3.684     3.575     1.886\n",
      ",    ,             543:     7.732     8.272     8.293\n",
      ".    ?             452:     2.990     2.758     3.444\n",
      "UH   yeah          251:     9.072     9.071     4.668\n",
      "``   \"             129:    21.941    10.461    22.343\n",
      ".    !             106:    27.632    24.242    17.505\n",
      "UH   okay           96:    35.975    36.529    28.550\n",
      "UH   oh             74:    29.677    32.334    15.586\n",
      "UH   ok             44:    76.303    76.931    63.879\n",
      ",    ...            41:   126.472   119.121   321.801\n",
      "UH   alright        40:    77.721    86.404    43.472\n",
      "UH   no             32:    88.057    77.502   161.361\n",
      ",    -              27:   123.160    95.764   248.680\n",
      "UH   huh            22:    43.995    47.158    62.636\n",
      ".    ..             16:   727.015   736.582  1356.636\n",
      "UH   yea            15:   249.345   121.621   140.079\n",
      "UH   uh             14:   244.629   263.408    15.227\n",
      ",    --             13:  2333.340  1668.593  1690.253\n",
      "UH   hi             12:   366.003   222.751   264.575\n",
      "UH   yeahh          12:   229.873   143.660    72.125\n",
      "UH   ah             10:   362.857   283.880   289.120\n",
      "UH   mm              9:   447.779   615.091   481.702\n",
      "UH   bah             9:    26.550    21.631   115.320\n",
      "NFP  _               8:     3.161     5.478     1.143\n",
      "UH   hey             7:   366.752   321.547   484.789\n",
      "UH   yep             7:   359.783   244.464   195.949\n",
      "UH   hello           6:   310.300   220.415   442.529\n",
      ",    ....            6:   419.821   494.280  1616.096\n",
      "UH   yay             6:   831.362   826.600   633.372\n",
      "UH   yeahhh          6:   384.427   254.595  1253.976\n",
      "UH   mmm             5:   595.780   558.999   383.932\n",
      "FW   nom             5:     5.191     4.583     5.960\n",
      "UH   yes             4:   729.713   605.349   441.761\n",
      ":    –               4: 23934.723  3980.566  2226.444\n",
      "UH   ohh             4:   625.566   616.271   385.339\n",
      "UH   yup             4:   727.383   543.644   369.395\n",
      "UH   bup             4:   184.218   462.438   940.261\n",
      ":    …               3:   486.139   124.472  6300.206\n",
      "UH   wow             3:   697.547   403.811   363.171\n",
      "UH   um              3:  1164.188   296.521   470.892\n",
      "UH   ahh             3:  1009.181  1519.832   407.984\n",
      "UH   op              3:  1353.470   812.362   497.581\n",
      "UH   hm              3:  1390.102  1001.236   966.605\n",
      "UH   ha              3:   763.097   922.091  2381.757\n",
      "UH   aw              3:  2923.291  6099.764   869.233\n",
      "UH   aww             3:  4086.681  8256.642  1641.325\n",
      "UH   wee             3:  1934.302  2400.975  1622.642\n",
      "UH   aha             3:  7184.315  2965.253 80848.660\n",
      "number of adjectives: 96\n",
      "JJ   right          51:    12.959    13.179    21.671\n",
      "JJR  more           38:    35.511    36.436    28.008\n",
      "JJ   good           30:    81.608    75.389    19.777\n",
      "JJ   little         28:    90.573    62.798    11.617\n",
      "JJ   big            16:   131.699   110.773   154.842\n",
      "JJ   ready          13:   336.787   235.521   178.364\n",
      "JJ   sure            9:    38.837    67.713   170.772\n",
      "JJ   other           8:   132.437   164.730    18.188\n",
      "JJ   else            8:     9.852    17.519     7.696\n",
      "JJ   lovely          7:    68.948    72.748   218.095\n",
      "JJ   happy           7:    28.731    18.870    40.639\n",
      "JJ   blue            7:   533.689   242.312   926.531\n",
      "JJ   hot             7:    35.818    38.854   352.767\n",
      "JJ   yellow          7:   693.250   201.578  2655.152\n",
      "JJ   black           6:  3351.567  3211.745  4254.350\n",
      "JJ   hard            5:    14.528     9.350    14.281\n",
      "JJ   red             5:   243.745   112.675   130.511\n",
      "JJ   nice            5:    78.187    86.762   265.856\n",
      "JJ   green           5:   307.608   146.117   245.993\n",
      "JJ   new             5:   563.347   607.247   156.077\n",
      "JJ   wrong           5:     7.829     5.205    63.480\n",
      "JJ   enough          5:   400.647   346.431  1716.564\n",
      "JJ   yummy           4:   247.014   107.748    22.861\n",
      "JJ   different       4:   228.721   196.366   298.429\n",
      "JJ   sorry           4:   478.958   605.216   284.893\n",
      "JJ   white           4:   476.609  1700.003  1527.527\n",
      "JJ   favorite        4:    12.205     7.835    12.991\n",
      "JJ   high            4:   501.599   358.462  1980.369\n",
      "JJ   much            3:    85.436    92.159   281.887\n",
      "JJ   cold            3:   178.929    55.792   129.783\n",
      "JJ   hungry          3:    38.209    27.493   174.566\n",
      "JJ   dark            3:    76.772    55.474   140.196\n",
      "JJ   same            3:    57.772    73.612   290.066\n",
      "JJ   wet             3:   367.272   254.725  1240.218\n",
      "JJ   careful         3:    20.822    21.376   340.198\n",
      "JJ   second          3:   109.334    84.814  5257.069\n",
      "JJ   orange          2:  2552.652  2420.023   140.510\n",
      "JJ   fine            2:    40.368    45.298   198.926\n",
      "JJ   last            2:   378.855   299.169    10.511\n",
      "JJ   few             2:    33.462    96.387    12.091\n",
      "JJ   old             2:  1562.009  4132.436   879.451\n",
      "JJ   interesting      2:  1057.024   974.356 23919.104\n",
      "JJ   tiny            2:  1769.743   787.492   308.497\n",
      "JJ   silky           2:     2.115     1.478     1.029\n",
      "JJ   brown           1:   124.817   326.403  2556.098\n",
      "JJ   sad             1:   243.385    24.815    42.973\n",
      "JJ   tired           1:   692.474   376.129    51.816\n",
      "JJ   many            1:    21.110    13.054     9.335\n",
      "JJ   long            1:  2601.001  2395.790  4805.817\n",
      "JJ   fluffy          1:  9449.647  6109.811  5538.008\n",
      "number of adverbs: 56\n",
      "RB   here           69:    57.211    44.916    52.490\n",
      "RB   now            53:    65.583    67.982   113.158\n",
      "RB   so             31:   208.682   224.207   236.567\n",
      "RB   too            27:   158.810   200.296   173.314\n",
      "RB   just           26:   118.337   106.042    92.592\n",
      "RB   very           23:    95.349    92.646    15.452\n",
      "RB   again          20:    72.626    76.701   167.122\n",
      "RB   then           17:    28.559    41.300    30.044\n",
      "RB   back           17:    12.191    11.836    42.868\n",
      "RB   well           12:    18.722    18.360    42.607\n",
      "RB   maybe          12:  1277.770   962.861  7138.541\n",
      "RB   pretty         11:   268.349   245.675    47.748\n",
      "RB   outside        10:    43.204    19.825    82.934\n",
      "RB   actually       10:  1003.700  1179.421  3895.318\n",
      "RB   away            9:    59.346    37.240    70.384\n",
      "RB   around          9:    13.524     9.972    77.687\n",
      "RB   really          8:   297.922   295.171   490.179\n",
      "RB   also            6:   350.829   418.347   424.224\n",
      "RB   only            6:  1025.227   950.755  1043.851\n",
      "RB   as              5:   591.400   522.018    18.846\n",
      "RB   inside          5:   157.764    71.676    95.098\n",
      "RB   almost          5:   366.682   497.598    82.122\n",
      "RB   far             4:   422.268   271.276    79.827\n",
      "RB   quite           4: 11442.972 19119.192 17911.801\n",
      "RB   still           3:   300.146   301.851   162.221\n",
      "RB   first           3:   237.939   243.823   480.273\n",
      "RBR  better          3:  1402.526  1512.057   579.730\n",
      "RB   yet             3:   690.921   275.269  1738.843\n",
      "RB   home            3:    25.197    11.842    73.293\n",
      "RB   somewhere       3:   521.704   376.496    65.121\n",
      "RB   never           3:   408.129   362.146   678.513\n",
      "RB   ahead           3:  7196.049 11183.270 26394.809\n",
      "RB   everywhere      3:  6076.872 11071.739  5702.080\n",
      "RB   sometimes       2:   899.427   293.523   495.097\n",
      "RB   probably        2:   134.320   249.973   432.748\n",
      "RB   either          2:  4899.639  4159.425  2113.605\n",
      "RB   once            2:  5606.420  1777.279  5871.599\n",
      "RB   instead         2: 16803.420 12793.757 14195.885\n",
      "RB   always          2:  1588.067  1242.902  4518.209\n",
      "RB   otherwise       2: 38499.008 31757.392 99999.990\n",
      "RB   anyway          2: 66083.579 38483.487 15897.581\n",
      "RB   though          1:   637.148   413.086  2254.945\n",
      "RB   together        1:  1308.329   702.957   274.205\n",
      "RB   before          1: 16071.938 22906.230 99999.990\n",
      "RB   kind            1:   292.901   372.046     3.953\n",
      "RB   soon            1:     6.146     5.796 36705.251\n",
      "RB   ever            1:  7288.282  3791.088    14.446\n",
      "RB   anyways         1:  2537.100  1753.543  5347.489\n",
      "RB   gladly          1:  2057.214   635.244  9329.024\n",
      "RB   ago             1:     5.407     8.602     1.646\n",
      "number of cardinal numbers: 17\n",
      "CD   two            12:    33.701    44.504    48.032\n",
      "CD   three          10:    56.278    42.870   257.205\n",
      "CD   four            6:     2.713     3.270   135.653\n",
      "CD   five            4:   480.333   231.656   239.830\n",
      "CD   seven           3:  1430.987   711.906   242.755\n",
      "CD   six             2:     3.133     6.745   106.368\n",
      "CD   eight           2:   103.488   222.900  3150.802\n",
      "CD   nine            2:    64.649    28.117   794.485\n",
      "CD   2               2:     2.493     3.326  1005.687\n",
      "CD   3               2:     1.775     2.086   950.458\n",
      "CD   1               2: 23838.008 18356.692  3006.938\n",
      "CD   ten             1:     3.654     3.954 31023.563\n",
      "CD   twenty          1: 11101.850  4085.142     3.404\n",
      "CD   4               1:    21.631    19.794 24741.816\n",
      "CD   hundred         1: 83154.500  6482.999    71.075\n",
      "CD   fifty           1: 75844.948 40778.096    39.911\n",
      "CD   5               1: 99999.990 15044.997 99999.990\n",
      "number of function words: 95\n",
      "PRP  you           512:     5.923     5.827     3.533\n",
      "DT   the           311:     8.950     8.374     4.620\n",
      "PRP  it            242:    11.236    10.751     9.312\n",
      "DT   a             238:     7.951     6.829     3.719\n",
      "DT   that          226:    24.554    22.002    10.493\n",
      "TO   to            220:     3.882     3.904     3.425\n",
      "CC   and           220:    16.097    15.848    14.038\n",
      "PRP  we            190:    16.151    14.660     5.782\n",
      "EX   there         167:    24.307    20.305    13.323\n",
      "PRP  i             162:    21.951    21.078     4.043\n",
      "IN   on            103:    12.724    13.816    15.023\n",
      "MD   can           102:    19.439    18.268     6.947\n",
      "IN   in             80:    16.574    17.582    23.074\n",
      "DT   this           77:    51.846    44.727    26.593\n",
      "PRP$ your           76:    14.835    17.467     4.714\n",
      "WP   what           72:    56.788    53.130    37.086\n",
      "DT   all            71:    52.810    48.143    15.484\n",
      "IN   of             58:     4.577     5.043     5.627\n",
      "DT   some           54:    18.276    15.861     8.245\n",
      "PRP  me             53:     7.483     7.711    15.688\n",
      "IN   with           43:    12.939    13.284    17.939\n",
      "PRP  they           43:    50.531    42.412    40.735\n",
      "IN   at             39:     4.376     4.312     5.886\n",
      "WRB  where          38:   108.009    83.534    31.618\n",
      "IN   for            36:    61.177    47.332    47.928\n",
      "RP   out            36:    19.359    19.446    43.021\n",
      "RP   up             32:    67.069    74.303   107.420\n",
      "PRP  he             30:    53.398    40.060    25.260\n",
      "MD   'll            30:    16.162    14.580    13.116\n",
      "CC   but            27:   102.054   120.246    73.580\n",
      "TO   na             27:     1.005     1.005     1.117\n",
      "IN   if             26:    94.125   118.744    64.724\n",
      "RP   off            23:    18.599    19.551   177.608\n",
      "DT   those          23:   148.958   129.611    88.123\n",
      "MD   will           22:    43.682    50.528    46.950\n",
      "PRP  them           21:    27.065    26.230    35.355\n",
      "WRB  how            21:   121.199   106.500    30.041\n",
      "MD   wanna          20:    85.498    84.122    39.376\n",
      "PRP$ my             18:    52.131    49.995     8.191\n",
      "IN   over           17:    92.553    65.305   153.323\n",
      "WRB  when           17:   400.767   249.746   277.994\n",
      "MD   should         15:   150.417   193.171    28.953\n",
      "DT   an             15:    59.785    70.892    37.351\n",
      "PRP$ its            15:   395.152   294.254   235.503\n",
      "WDT  which          15:   339.632   226.039    45.427\n",
      "CC   or             13:   165.597   184.853   199.343\n",
      "RP   down           13:   109.494    51.362   269.290\n",
      "DT   another        12:   389.442   476.753    67.480\n",
      "WRB  why            12:   409.245   471.526   211.801\n",
      "MD   ca             12:   115.082   113.318     7.404\n",
      "number of nouns: 443\n",
      "NN   ball           31:    14.225     6.112    29.382\n",
      "NNP  sam            24:    92.638    92.004   203.283\n",
      "NN   kitty          23:    33.025    25.087    57.258\n",
      "NN   baby           23:    29.404    13.492    38.158\n",
      "NN   book           16:    78.658    70.119   212.209\n",
      "NN   water          16:    87.121    57.074   114.019\n",
      "NN   bear           15:   139.987   113.753   116.725\n",
      "NN   car            15:    41.847    70.751   259.191\n",
      "NNS  shoes          15:    32.458    18.069   160.296\n",
      "NN   train          13:    58.353    20.161    80.916\n",
      "NN   bop            13:     5.952     5.535     4.772\n",
      "NN   today          12:   176.107   149.046  2605.937\n",
      "NN   page           10:    59.972    22.065   327.976\n",
      "NN   camera         10:   168.247   158.705   273.446\n",
      "NN   brush          10:    21.841    12.043   104.987\n",
      "NN   time            9:   110.684   117.983   201.567\n",
      "NN   mommy           9:   476.802   607.823   552.403\n",
      "NNS  balls           9:    90.251    23.203    94.307\n",
      "NN   banana          8:    72.186    38.521   178.479\n",
      "NN   way             8:    52.912    36.373   354.730\n",
      "NN   job             8:     1.902     1.635     2.159\n",
      "NN   boy             8:   296.122   282.042   255.577\n",
      "NN   bread           7:   193.730   193.434   400.373\n",
      "NNS  eggs            7:    85.701    35.914   241.214\n",
      "NN   potty           7:   149.325    66.881   489.367\n",
      "NN   bucket          7:    50.694    23.508   619.989\n",
      "NNS  cars            7:    55.704    33.884   371.472\n",
      "NN   moop            7:     2.352     1.961     1.169\n",
      "NN   doggy           6:   119.091    35.331   119.260\n",
      "NNS  socks           6:    49.652    30.645    78.329\n",
      "NN   something       6:   310.720   278.871    52.905\n",
      "NNP  bin             6:    90.626    66.845   157.544\n",
      "NN   nappy           6:    54.677    27.592   107.299\n",
      "NN   egg             6:    43.152    21.020   231.138\n",
      "NN   duck            6:   148.569    80.512   127.773\n",
      "NN   girl            6:   114.196    72.774   315.525\n",
      "NN   beach           6:    14.927    16.292   152.666\n",
      "NN   bloop           6:    65.832    48.357    41.762\n",
      "NN   light           6:   149.578    86.191   403.206\n",
      "NN   jacket          6:    48.109    29.796   127.396\n",
      "NNS  trees           6:   232.799   125.927   585.406\n",
      "NN   bowl            6:    24.960    15.210   194.761\n",
      "NN   blo             6:     6.061    12.393     3.264\n",
      "NN   poo             5:   774.660   171.893  1334.495\n",
      "NN   truck           5:     6.910     8.403    13.058\n",
      "NNS  things          5:   485.874   497.914   503.548\n",
      "NN   bunny           5:   144.607    57.332   352.898\n",
      "NN   guy             5:     1.199     1.212     7.902\n",
      "NN   hand            5:  1318.411  3657.166  2338.917\n",
      "NN   bottle          5:  1069.940   775.093  1968.566\n",
      "number of verbs: 236\n",
      "VBP  do            153:    21.250    20.322    10.201\n",
      "VBP  want          123:     4.310     4.413     4.668\n",
      "VB   go             96:     4.582     4.652    18.682\n",
      "VBP  have           94:    18.977    16.788    14.460\n",
      "VB   put            73:    21.115    18.208    21.998\n",
      "VB   get            60:    22.630    21.325    25.275\n",
      "VB   see            57:    35.424    30.880    53.395\n",
      "VB   let            46:    51.423    49.812    10.457\n",
      "VBG  going          46:    13.958    13.492    12.809\n",
      "VBP  know           37:     7.827     9.098    14.818\n",
      "VB   look           36:   157.889   132.428    13.049\n",
      "VBP  like           34:    46.457    37.059    58.197\n",
      "VBP  think          30:    10.705     9.224    22.100\n",
      "VBN  done           29:     8.194     8.494    17.465\n",
      "VBG  gon            27:    14.822    14.099     1.118\n",
      "VB   come           22:   172.899   167.711    49.040\n",
      "VBD  did            18:   295.589   291.945   188.708\n",
      "VB   make           16:   109.013    93.925   182.844\n",
      "VB   play           15:   152.631   113.502    14.972\n",
      "VBZ  goes           15:    66.751    50.011   311.015\n",
      "VB   try            14:    32.675    28.760    40.078\n",
      "VB   turn           14:    48.437    42.399    31.376\n",
      "VBP  need           13:    96.448    87.867    34.510\n",
      "VB   walk           12:    76.482    42.217    71.004\n",
      "VBD  got            11:    39.337    43.617    28.170\n",
      "VBZ  does           11:   234.353   248.610    61.939\n",
      "VBN  gone           11:    12.901    12.500    77.945\n",
      "VBG  doing          11:    76.740    68.909   247.193\n",
      "VB   draw           10:   251.205    54.298   126.147\n",
      "VB   push           10:    96.706   131.541  1138.891\n",
      "VB   take            9:   175.022   230.653    67.152\n",
      "VB   give            9:    65.089    44.432    28.517\n",
      "VBZ  has             9:    31.487    22.168    53.392\n",
      "VB   say             9:   186.248   205.752    80.351\n",
      "VB   sit             9:   123.094   106.469   131.955\n",
      "VB   clap            9:    35.975    45.689   547.204\n",
      "VB   show            9:    35.572    25.340    52.088\n",
      "VB   throw           9:   227.073   102.604   323.332\n",
      "VB   close           9:   480.919   377.054   510.475\n",
      "VBP  thank           9:   407.666   274.506   358.285\n",
      "VB   read            8:    30.794     7.342    60.564\n",
      "VB   help            8:   168.949   186.893    81.726\n",
      "VB   eat             7:   177.565   107.500   819.087\n",
      "VBP  've             7:   139.984   117.842    73.996\n",
      "VB   open            7:   347.705   375.465   400.370\n",
      "VB   hold            6:   190.184   149.232   284.692\n",
      "VB   pick            6:   260.949    80.594   142.929\n",
      "VB   clean           6:   520.655   183.503   751.177\n",
      "VB   remember        5:   117.393   106.937   162.141\n",
      "VBG  eating          5:    41.465    34.722   369.799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".    <sos>        1874:     1.000     1.000     9.994\n",
      ".    <eos>        1857:     1.556     1.526     3.042\n",
      ".    .             941:     3.684     3.575     1.886\n",
      ",    ,             543:     7.732     8.272     8.293\n",
      "PRP  you           512:     5.923     5.827     3.533\n",
      ".    ?             452:     2.990     2.758     3.444\n",
      "DT   the           311:     8.950     8.374     4.620\n",
      "UH   yeah          251:     9.072     9.071     4.668\n",
      "PRP  it            242:    11.236    10.751     9.312\n",
      "DT   a             238:     7.951     6.829     3.719\n",
      "DT   that          226:    24.554    22.002    10.493\n",
      "TO   to            220:     3.882     3.904     3.425\n",
      "CC   and           220:    16.097    15.848    14.038\n",
      "PRP  we            190:    16.151    14.660     5.782\n",
      "EX   there         167:    24.307    20.305    13.323\n",
      "PRP  i             162:    21.951    21.078     4.043\n",
      "VBP  do            153:    21.250    20.322    10.201\n",
      "``   \"             129:    21.941    10.461    22.343\n",
      "VBP  want          123:     4.310     4.413     4.668\n",
      ".    !             106:    27.632    24.242    17.505\n",
      "IN   on            103:    12.724    13.816    15.023\n",
      "MD   can           102:    19.439    18.268     6.947\n",
      "VB   go             96:     4.582     4.652    18.682\n",
      "UH   okay           96:    35.975    36.529    28.550\n",
      "VBP  have           94:    18.977    16.788    14.460\n",
      "IN   in             80:    16.574    17.582    23.074\n",
      "DT   this           77:    51.846    44.727    26.593\n",
      "PRP$ your           76:    14.835    17.467     4.714\n",
      "UH   oh             74:    29.677    32.334    15.586\n",
      "VB   put            73:    21.115    18.208    21.998\n",
      "WP   what           72:    56.788    53.130    37.086\n",
      "DT   all            71:    52.810    48.143    15.484\n",
      "RB   here           69:    57.211    44.916    52.490\n",
      "VB   get            60:    22.630    21.325    25.275\n",
      "IN   of             58:     4.577     5.043     5.627\n",
      "VB   see            57:    35.424    30.880    53.395\n",
      "DT   some           54:    18.276    15.861     8.245\n",
      "RB   now            53:    65.583    67.982   113.158\n",
      "PRP  me             53:     7.483     7.711    15.688\n",
      "JJ   right          51:    12.959    13.179    21.671\n",
      "VB   let            46:    51.423    49.812    10.457\n",
      "VBG  going          46:    13.958    13.492    12.809\n",
      "UH   ok             44:    76.303    76.931    63.879\n",
      "IN   with           43:    12.939    13.284    17.939\n",
      "PRP  they           43:    50.531    42.412    40.735\n",
      ",    ...            41:   126.472   119.121   321.801\n",
      "UH   alright        40:    77.721    86.404    43.472\n",
      "IN   at             39:     4.376     4.312     5.886\n",
      "WRB  where          38:   108.009    83.534    31.618\n",
      "JJR  more           38:    35.511    36.436    28.008\n",
      "VBP  know           37:     7.827     9.098    14.818\n",
      "VB   look           36:   157.889   132.428    13.049\n",
      "IN   for            36:    61.177    47.332    47.928\n",
      "RP   out            36:    19.359    19.446    43.021\n",
      "VBP  like           34:    46.457    37.059    58.197\n",
      "UH   no             32:    88.057    77.502   161.361\n",
      "RP   up             32:    67.069    74.303   107.420\n",
      "RB   so             31:   208.682   224.207   236.567\n",
      "NN   ball           31:    14.225     6.112    29.382\n",
      "PRP  he             30:    53.398    40.060    25.260\n",
      "VBP  think          30:    10.705     9.224    22.100\n",
      "JJ   good           30:    81.608    75.389    19.777\n",
      "MD   'll            30:    16.162    14.580    13.116\n",
      "VBN  done           29:     8.194     8.494    17.465\n",
      "JJ   little         28:    90.573    62.798    11.617\n",
      "CC   but            27:   102.054   120.246    73.580\n",
      ",    -              27:   123.160    95.764   248.680\n",
      "RB   too            27:   158.810   200.296   173.314\n",
      "TO   na             27:     1.005     1.005     1.117\n",
      "VBG  gon            27:    14.822    14.099     1.118\n",
      "RB   just           26:   118.337   106.042    92.592\n",
      "IN   if             26:    94.125   118.744    64.724\n",
      "NNP  sam            24:    92.638    92.004   203.283\n",
      "NN   kitty          23:    33.025    25.087    57.258\n",
      "RB   very           23:    95.349    92.646    15.452\n",
      "NN   baby           23:    29.404    13.492    38.158\n",
      "RP   off            23:    18.599    19.551   177.608\n",
      "DT   those          23:   148.958   129.611    88.123\n",
      "MD   will           22:    43.682    50.528    46.950\n",
      "VB   come           22:   172.899   167.711    49.040\n",
      "UH   huh            22:    43.995    47.158    62.636\n",
      "PRP  them           21:    27.065    26.230    35.355\n",
      "WRB  how            21:   121.199   106.500    30.041\n",
      "RB   again          20:    72.626    76.701   167.122\n",
      "MD   wanna          20:    85.498    84.122    39.376\n",
      "PRP$ my             18:    52.131    49.995     8.191\n",
      "VBD  did            18:   295.589   291.945   188.708\n",
      "RB   then           17:    28.559    41.300    30.044\n",
      "RB   back           17:    12.191    11.836    42.868\n",
      "IN   over           17:    92.553    65.305   153.323\n",
      "WRB  when           17:   400.767   249.746   277.994\n",
      "JJ   big            16:   131.699   110.773   154.842\n",
      "NN   book           16:    78.658    70.119   212.209\n",
      "VB   make           16:   109.013    93.925   182.844\n",
      ".    ..             16:   727.015   736.582  1356.636\n",
      "NN   water          16:    87.121    57.074   114.019\n",
      "VB   play           15:   152.631   113.502    14.972\n",
      "MD   should         15:   150.417   193.171    28.953\n",
      "DT   an             15:    59.785    70.892    37.351\n",
      "UH   yea            15:   249.345   121.621   140.079\n",
      "\n",
      "VB   look           36:   157.889   132.428    13.049\n",
      "VBP  need           13:    96.448    87.867    34.510\n",
      "VB   draw           10:   251.205    54.298   126.147\n"
     ]
    }
   ],
   "source": [
    "split_pos_items, split_pos_pos_items, split_all_pos_items = {}, {}, {}\n",
    "\n",
    "for split in used_splits:\n",
    "    print(f'{split} split:')\n",
    "\n",
    "    top_token_items = split_items[split].token_items.sort_values('cnt', ascending=False, kind='stable')   # sort by cnt\n",
    "    top_token_items = top_token_items[~top_token_items[token_field].isin(untypical_words)]  # remove untypical words\n",
    "    pos_field = 'syntactic category'\n",
    "    used_poses = top_token_items.dtypes[pos_field].categories\n",
    "    pos_items = {pos: top_token_items[top_token_items[pos_field] == pos] for pos in used_poses}\n",
    "    split_pos_items[split] = pos_items\n",
    "    for pos in used_poses:\n",
    "        items = pos_items[pos]\n",
    "        print(f'number of {pos}s: {len(items)}')\n",
    "        for _, row in items[:50].iterrows():\n",
    "            print(row_str(row, names))\n",
    "\n",
    "    interleaving_step = 1\n",
    "\n",
    "    pos_pos_items = {}\n",
    "    for i_pos in range(len(used_poses)):\n",
    "        for j_pos in range(len(used_poses)):\n",
    "            if i_pos != j_pos:\n",
    "                pos_i = used_poses[i_pos]\n",
    "                pos_i_items = pos_items[pos_i]\n",
    "                pos_j = used_poses[j_pos]\n",
    "                pos_j_items = pos_items[pos_j]\n",
    "                interleaved_dfs = []\n",
    "                i = -interleaving_step\n",
    "                for i in range(0, min(len(pos_i_items), len(pos_j_items)), interleaving_step):\n",
    "                    interleaved_dfs.append(pos_i_items[i:i+interleaving_step])\n",
    "                    interleaved_dfs.append(pos_j_items[i:i+interleaving_step])\n",
    "                else:\n",
    "                    i += interleaving_step\n",
    "                    interleaved_dfs.append(pos_i_items[i:])\n",
    "                    interleaved_dfs.append(pos_j_items[i:])\n",
    "                pos_pos_items[(pos_i, pos_j)] = pd.concat(interleaved_dfs)\n",
    "    split_pos_pos_items[split] = pos_pos_items\n",
    "\n",
    "    interleaved_dfs = []\n",
    "    for i in range(0, max(map(len, pos_items.values())), interleaving_step):\n",
    "        for pos in used_poses:\n",
    "            items = pos_items[pos]\n",
    "            interleaved_dfs.append(items[i:i+interleaving_step])\n",
    "    all_pos_items = pd.concat(interleaved_dfs)\n",
    "    split_all_pos_items[split] = all_pos_items\n",
    "\n",
    "    # check some items\n",
    "    for _, row in top_token_items[:100].iterrows():\n",
    "        print(row_str(row, names))\n",
    "    print()\n",
    "    for word in ['look', 'need', 'draw']:\n",
    "        try:\n",
    "            token_id = word2idx[word]\n",
    "            for _, row in top_token_items.loc[token_id].iterrows():\n",
    "                print(row_str(row, names))\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW:\n",
      "saving plot Embedding Noun vs Verb RSA\n",
      "LSTM:\n",
      "saving plot LSTM Embedding Noun vs Verb Dendrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/seaborn/matrix.py:1221: UserWarning: ``square=True`` ignored in clustermap\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving plot LSTM Embedding Noun vs Verb Similarity Heatmap\n",
      "LSTM Captioning:\n",
      "saving plot LSTM Captioning Embedding Noun vs Verb Dendrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/seaborn/matrix.py:1221: UserWarning: ``square=True`` ignored in clustermap\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving plot LSTM Captioning Embedding Noun vs Verb Similarity Heatmap\n",
      "CBOW:\n",
      "saving plot CBOW Embedding Noun vs Verb Dendrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/seaborn/matrix.py:1221: UserWarning: ``square=True`` ignored in clustermap\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving plot CBOW Embedding Noun vs Verb Similarity Heatmap\n",
      "CBOW:\n",
      "saving plot Embedding Semantic Categories RSA\n",
      "LSTM:\n",
      "saving plot LSTM Embedding Semantic Categories Dendrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/seaborn/matrix.py:1221: UserWarning: ``square=True`` ignored in clustermap\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving plot LSTM Embedding Semantic Categories Similarity Heatmap\n",
      "LSTM Captioning:\n",
      "saving plot LSTM Captioning Embedding Semantic Categories Dendrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/seaborn/matrix.py:1221: UserWarning: ``square=True`` ignored in clustermap\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving plot LSTM Captioning Embedding Semantic Categories Similarity Heatmap\n",
      "CBOW:\n",
      "saving plot CBOW Embedding Semantic Categories Dendrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/seaborn/matrix.py:1221: UserWarning: ``square=True`` ignored in clustermap\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving plot CBOW Embedding Semantic Categories Similarity Heatmap\n",
      "CBOW:\n",
      "saving plot Embedding Verb Transitivity RSA\n",
      "LSTM:\n",
      "saving plot LSTM Embedding Verb Transitivity Dendrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/seaborn/matrix.py:1221: UserWarning: ``square=True`` ignored in clustermap\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving plot LSTM Embedding Verb Transitivity Similarity Heatmap\n",
      "LSTM Captioning:\n",
      "saving plot LSTM Captioning Embedding Verb Transitivity Dendrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/seaborn/matrix.py:1221: UserWarning: ``square=True`` ignored in clustermap\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving plot LSTM Captioning Embedding Verb Transitivity Similarity Heatmap\n",
      "CBOW:\n",
      "saving plot CBOW Embedding Verb Transitivity Dendrogram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ww2135/.local/lib/python3.9/site-packages/seaborn/matrix.py:1221: UserWarning: ``square=True`` ignored in clustermap\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving plot CBOW Embedding Verb Transitivity Similarity Heatmap\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x2250 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x2400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x2250 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x2400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x2250 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x2400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x2400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x2160 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x2250 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x2400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x2250 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x2400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x2250 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x2400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_dendrogram(\n",
    "    items,\n",
    "    names,\n",
    "    vector_attr='mean_vector',\n",
    "    heatmap=False,\n",
    "    annot=False,\n",
    "    size=0.7,\n",
    "    color_threshold=None,\n",
    "    heatmap_linkage=None,\n",
    "    tag_field='POS tag',\n",
    "    ctg_field='POS tag',\n",
    "    ll_tag_field='pos',\n",
    "    ll_with_cnt=True,\n",
    "    ll_with_ppl=True,\n",
    "    title=None\n",
    "):\n",
    "    \"\"\"linkage clustering and dendrogram plotting\n",
    "    items: pd.DataFrame\n",
    "    names: the names of the models to plot\n",
    "    vector_attr: use value.vector_attr; default: 'mean_vector'; can be 'embedding'\n",
    "    heatmap: bool, plot something like plot_sim_heatmap\n",
    "    heatmap_linkage: the row_linkage and col_linkage used in heatmap; can be any of:\n",
    "        None: use the linkage result from the vectors of current model\n",
    "        \"first\": use the linkage result of the first model\n",
    "        \"tag\": build the linkage by clustering items by tag_field\n",
    "        result from linkage function\n",
    "    tag_field: the field of tags to obtain the palette of the sidebar of heatmaps\n",
    "    ctg_field: the field as the category in dendrograms\n",
    "    ll_tag_field: the field of tags to append to leaf labels; set to None or empty string if unwanted\n",
    "    ll_with_cnt: whether to append cnt to leaf labels\n",
    "    ll_with_ppl: whether to append ppl to leaf labels\n",
    "    title: title of the plots\n",
    "    \"\"\"\n",
    "    from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "    n_items = len(items)\n",
    "    vectors = [get_np_attrs_from_values(items[name], vector_attr) for name in names]\n",
    "\n",
    "    title_ = f'{title} RSA'\n",
    "    plot_repres_sim_heatmap(vectors, names, title=title_)\n",
    "    output_fig(title_)\n",
    "\n",
    "    # build color map\n",
    "    colors = items[tag_field].astype('O').map(get_palette(tag_field)).tolist()\n",
    "\n",
    "    if heatmap:\n",
    "        if heatmap_linkage == \"tag\":  # build Z_heatmap based on tag_field\n",
    "            Z_heatmap = build_linkage_by_same_value(items[tag_field])\n",
    "        elif not (heatmap_linkage is None or isinstance(heatmap_linkage, str)):  # use heatmap_linkage\n",
    "            Z_heatmap = heatmap_linkage\n",
    "\n",
    "    ll_tag_width = max(map(len, items[ll_tag_field]))\n",
    "\n",
    "    for n, (name, V) in enumerate(zip(names, vectors)):\n",
    "        print(f'{name}:')\n",
    "        Z = linkage(V, method='average', metric='cosine')  # of shape (number of merges = n_items - 1, 4)\n",
    "\n",
    "        def llf(index):\n",
    "            if index < n_items:\n",
    "                row = items.iloc[index]\n",
    "                return row_llf(\n",
    "                    items.iloc[index],\n",
    "                    tag_field=ll_tag_field,\n",
    "                    tag_width=ll_tag_width,\n",
    "                    sep='  ',\n",
    "                    with_cnt=ll_with_cnt,\n",
    "                    name=name if ll_with_ppl else None,\n",
    "                    baseline_name=None if n == 0 else names[0],\n",
    "                )\n",
    "            else:\n",
    "                merge_index = index - n_items\n",
    "                return f'{merge_index} {int(Z[merge_index, 3])} {Z[merge_index, 2]:.3f}'\n",
    "\n",
    "        ctg_palette = get_palette(ctg_field)\n",
    "        ctg_sets = [{ctg} for ctg in items[ctg_field]]\n",
    "        link_colors = []\n",
    "        for link in Z:\n",
    "            ctg_set = ctg_sets[int(link[0])] | ctg_sets[int(link[1])]\n",
    "            ctg_sets.append(ctg_set)\n",
    "            if len(ctg_set) == 1:\n",
    "                ctg = next(iter(ctg_set))\n",
    "                link_color = ctg_palette[ctg]\n",
    "            else:\n",
    "                link_color = 'black'\n",
    "            link_colors.append(link_color)\n",
    "\n",
    "        p = 10000\n",
    "\n",
    "        plt.figure(figsize=(4, 0.15 * min(p, n_items))) # 0.1\n",
    "        dendrogram(\n",
    "            Z,\n",
    "            truncate_mode='lastp',\n",
    "            p=p,\n",
    "            color_threshold=color_threshold,\n",
    "            orientation='left',\n",
    "            leaf_rotation=0.,\n",
    "            #leaf_font_size=16.,\n",
    "            leaf_label_func=llf,\n",
    "            link_color_func=lambda k: link_colors[k - n_items],\n",
    "        )\n",
    "\n",
    "        title_ = f\"{name} {title} Dendrogram\"\n",
    "        if title is not None:\n",
    "            plt.title(title_)\n",
    "        #plt.show()\n",
    "        output_fig(title_)\n",
    "\n",
    "        if heatmap:\n",
    "            if heatmap_linkage is None:\n",
    "                Z_heatmap = Z\n",
    "            elif heatmap_linkage == \"first\":\n",
    "                if n == 0:\n",
    "                    Z_heatmap = Z\n",
    "\n",
    "            prefix_labels = [row_llf(row, tag_field=tag_field, with_cnt=False) for _, row in items.iterrows()]\n",
    "            llf_labels = list(map(llf, range(n_items)))\n",
    "\n",
    "            matrix = cosine_matrix(V)\n",
    "\n",
    "            off_diag = ~np.eye(matrix.shape[0], matrix.shape[1], dtype=bool)\n",
    "            v = np.max(np.abs(matrix[off_diag]))\n",
    "            vmin = -v\n",
    "            vmax = +v\n",
    "\n",
    "            g = sns.clustermap(\n",
    "                matrix,\n",
    "                row_linkage=Z_heatmap,\n",
    "                col_linkage=Z_heatmap,\n",
    "                figsize=(8, 8),\n",
    "                cbar_pos=None,\n",
    "                # kwargs for heatmap\n",
    "                vmin=vmin, vmax=vmax, center=0,\n",
    "                annot=annot, fmt='.2f',\n",
    "                xticklabels=prefix_labels,\n",
    "                yticklabels=prefix_labels,\n",
    "                row_colors=colors,\n",
    "                col_colors=colors,\n",
    "                square=True,\n",
    "                #cbar=False,\n",
    "                dendrogram_ratio=0., # remove all dendrograms\n",
    "                colors_ratio=0.02,\n",
    "            )\n",
    "            g.ax_col_dendrogram.remove()\n",
    "\n",
    "            title_ = f\"{name} {title} Similarity Heatmap\"\n",
    "            if title is not None:\n",
    "                plt.title(title_)\n",
    "            output_fig(title_)\n",
    "\n",
    "\n",
    "def get_subcat_items(items, pos, n_items_from_each_cat=None):\n",
    "    if n_items_from_each_cat is None:\n",
    "        n_items_from_each_cat = {'noun': 6, 'verb': 25}[pos]\n",
    "    dfs = []\n",
    "    for cat_name, words in pos_subcats[pos].items():\n",
    "        df = items[items[token_field].isin(words)].copy()\n",
    "        if len(df) >= n_items_from_each_cat:\n",
    "            dfs.append(df[:n_items_from_each_cat])\n",
    "    items = pd.concat(dfs)\n",
    "    items[subcat_field] = items[token_field].map(word2subcat).astype('category')\n",
    "    return items\n",
    "\n",
    "\n",
    "for split in ['train']:\n",
    "    for title, items, tag_field, ctg_field, ll_tag_field, heatmap_linkage in (\n",
    "        ('Most Frequent Words', split_all_pos_items[split][:100], 'POS tag', 'POS tag', 'pos', 'first'),\n",
    "        ('Noun vs Verb', split_pos_pos_items[split][('noun', 'verb')][:50], 'POS tag', 'syntactic category', 'pos', 'first'),\n",
    "        ('Semantic Categories', get_subcat_items(split_pos_items[split]['noun'], 'noun'), subcat_field, subcat_field, subcat_field, 'tag'),\n",
    "        ('Verb Transitivity', get_subcat_items(split_pos_items[split]['verb'], 'verb'), subcat_field, subcat_field, subcat_field, 'tag'),\n",
    "    )[1:]:\n",
    "        print(f'{name}:')\n",
    "        with sns.axes_style(heatmap_style, rc={'font.family': [font]}):\n",
    "            plot_dendrogram(\n",
    "                items,\n",
    "                names,\n",
    "                heatmap=True,\n",
    "                heatmap_linkage=heatmap_linkage,\n",
    "                title=f'{repres_name} {title}',\n",
    "                vector_attr=vector_attr,\n",
    "                tag_field=tag_field,\n",
    "                ctg_field=ctg_field,\n",
    "                ll_tag_field=ll_tag_field,\n",
    "                ll_with_cnt=False,\n",
    "                ll_with_ppl=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type level:\n",
      ".               #: 65\n",
      "adjective       #: 44\n",
      "adverb          #: 45\n",
      "cardinal number #: 11\n",
      "function word   #: 82\n",
      "noun            #: 220\n",
      "verb            #: 150\n",
      "plotting 617/617 = 100.00% items...\n",
      "saving plot Type Level Loss Difference\n",
      "LSTM Captioning:\n",
      "plotting 617/617 = 100.00% items...\n",
      "saving plot LSTM Captioning Type Level Loss Difference Distribution\n",
      ".               #examples:    65 mean: -0.16 t-test result: statistic:  -2.09 pvalue: 0.04\n",
      "adjective       #examples:    44 mean: -0.14 t-test result: statistic:  -1.75 pvalue: 0.09\n",
      "adverb          #examples:    45 mean: -0.14 t-test result: statistic:  -2.23 pvalue: 0.03\n",
      "cardinal number #examples:    11 mean: -0.03 t-test result: statistic:  -0.18 pvalue: 0.86\n",
      "function word   #examples:    82 mean: -0.13 t-test result: statistic:  -3.25 pvalue: 0.00\n",
      "noun            #examples:   220 mean: -0.51 t-test result: statistic:  -9.44 pvalue: 0.00\n",
      "verb            #examples:   150 mean: -0.29 t-test result: statistic:  -5.58 pvalue: 0.00\n",
      "all tokens      #examples:   617 mean: -0.31 t-test result: statistic: -11.42 pvalue: 0.00\n",
      "====================================================================================================\n",
      "Token level:\n",
      ".               #: 6818\n",
      "adjective       #: 413\n",
      "adverb          #: 740\n",
      "cardinal number #: 80\n",
      "function word   #: 4042\n",
      "noun            #: 1406\n",
      "verb            #: 2346\n",
      "plotting 15845/15845 = 100.00% items...\n",
      "saving plot Token Level Loss Difference\n",
      "LSTM Captioning:\n",
      ".               #examples:  6818 mean: -0.03 t-test result: statistic:  -6.76 pvalue: 0.00\n",
      "adjective       #examples:   413 mean: -0.15 t-test result: statistic:  -3.53 pvalue: 0.00\n",
      "adverb          #examples:   740 mean: -0.07 t-test result: statistic:  -3.14 pvalue: 0.00\n",
      "cardinal number #examples:    80 mean: -0.06 t-test result: statistic:  -0.58 pvalue: 0.56\n",
      "function word   #examples:  4042 mean: -0.06 t-test result: statistic:  -7.00 pvalue: 0.00\n",
      "noun            #examples:  1406 mean: -0.42 t-test result: statistic: -14.96 pvalue: 0.00\n",
      "verb            #examples:  2346 mean: -0.11 t-test result: statistic:  -8.64 pvalue: 0.00\n",
      "all tokens      #examples: 15845 mean: -0.09 t-test result: statistic: -19.27 pvalue: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "\n",
    "def get_best_split_accuracy(label_0, label_1):\n",
    "    n_label_0 = label_0.value_counts()[True]\n",
    "    n_label_1 = label_1.value_counts()[True]\n",
    "\n",
    "    i = 0\n",
    "    count_0 = 0\n",
    "    count_1 = n_label_1\n",
    "\n",
    "    best_i, best_n = i, count_0 + count_1\n",
    "\n",
    "    for label_i_0, label_i_1 in zip(label_0, label_1):\n",
    "        count_0 += label_i_0\n",
    "        count_1 -= label_i_1\n",
    "        i += 1\n",
    "\n",
    "        if count_0 + count_1 > best_n:\n",
    "            best_i, best_n = i, count_0 + count_1\n",
    "\n",
    "    print(f'n_label_0: {n_label_0}, n_label_1: {n_label_1}, best_accuracy: {best_n / (n_label_0 + n_label_1):.2%}')\n",
    "    return best_i, best_n / (n_label_0 + n_label_1)\n",
    "\n",
    "\n",
    "def plot_ROC(y_true, y_score, pos_label=None, label=\"\", **kwargs):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=pos_label)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        label=f\"{label} (AUC = {roc_auc:0.2f})\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_ROC_end(title=\"\", **kwargs):\n",
    "    plt.plot([0, 1], [0, 1], **kwargs)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    output_fig(title)\n",
    "\n",
    "\n",
    "def ttest(a, label, alternative):\n",
    "    result = ttest_rel(a, np.zeros_like(a), alternative=alternative)\n",
    "    print(f'{label:15} #examples: {len(a):5} mean: {np.mean(a):5.2f} t-test result: statistic: {result.statistic:6.2f} pvalue: {result.pvalue:4.2f}')\n",
    "\n",
    "\n",
    "def analyze_value(\n",
    "    items,\n",
    "    value_attr,\n",
    "    model_names,\n",
    "    cat_field=\"syntactic category\",\n",
    "    palette=None,\n",
    "    title=None,\n",
    "    hlines=None,\n",
    "    find_best_threshold=False,\n",
    "    plotting=True,\n",
    "    ROC=False,\n",
    "    ttest_alternative='two-sided',\n",
    "):\n",
    "    \"\"\"\n",
    "    items: items\n",
    "    value_attr: column in items; e.g., 'loss', 'prob'\n",
    "    model_names: names of the models to analyze\n",
    "    cat_field: the category field\n",
    "    palette: the palette for the categories; a dict mapping each category name to a color\n",
    "    \"\"\"\n",
    "    if palette is None:\n",
    "        palette = get_palette(cat_field)\n",
    "    value_suffix = ' ' + value_attr\n",
    "    model_field = 'model'\n",
    "    if title is None:\n",
    "        title = value_attr\n",
    "\n",
    "    for cat in items.dtypes[cat_field].categories:\n",
    "        cat_items = items[items[cat_field] == cat]\n",
    "        if len(cat_items) > 0:\n",
    "            print(f'{cat:15} #: {len(cat_items)}')\n",
    "    items_long = items.melt(\n",
    "        id_vars=[cat_field],\n",
    "        value_vars=[name + value_suffix for name in model_names],\n",
    "        var_name=model_field,\n",
    "        value_name=value_attr)\n",
    "    items_long[model_field] = items_long[model_field].map(lambda s: s[:-len(value_suffix)])\n",
    "    plot(\n",
    "        sns.catplot,\n",
    "        items_long,\n",
    "        x=model_field,\n",
    "        y=value_attr,\n",
    "        hue=cat_field,\n",
    "        palette=palette,\n",
    "        hlines=hlines,\n",
    "        kind=\"box\",\n",
    "        figsize=(8, 4),\n",
    "        medianprops=dict(color=\"white\", alpha=0.7),\n",
    "        title=title)\n",
    "    output_fig(title)\n",
    "\n",
    "    for name in model_names:\n",
    "        print(f'{name}:')\n",
    "        value_field = f\"{name} {value_attr}\"\n",
    "        cur_items = items.sort_values(value_field)\n",
    "\n",
    "        if find_best_threshold:\n",
    "            assert cat_field == \"syntactic category\"\n",
    "            label_0 = cur_items[cat_field].isin([\"noun\"])\n",
    "            label_1 = ~label_0 #cur_items[cat_field].isin([\"function word\", \"adjective\", \"adverb\", \"cardinal number\"])\n",
    "            best_split_i, best_split_acc = get_best_split_accuracy(label_0, label_1)\n",
    "            print(f'best_split_accuracy: {best_split_acc:.2%}')\n",
    "            threshold = cur_items.iloc[min(best_split_i, len(cur_items)-1)][value_field]\n",
    "\n",
    "        if plotting:\n",
    "            _title = f'{name} {title} Distribution'\n",
    "            plot(\n",
    "                sns.kdeplot,\n",
    "                cur_items,\n",
    "                x=value_field,\n",
    "                hue=cat_field,\n",
    "                palette=palette,\n",
    "                bw_adjust=.5,\n",
    "                figsize=(8, 4),\n",
    "                title=_title)\n",
    "            output_fig(_title)\n",
    "\n",
    "            if ROC:\n",
    "                for cat in cur_items.dtypes[cat_field].categories:\n",
    "                    plot_ROC(\n",
    "                        cur_items[cat_field],\n",
    "                        -cur_items[value_field],\n",
    "                        pos_label=cat,\n",
    "                        label=f\"{cat} vs others\",\n",
    "                        color=palette[cat])\n",
    "                plot_ROC_end(title=_title + 'ROC')\n",
    "\n",
    "        for cat in cur_items.dtypes[cat_field].categories:\n",
    "            ttest(cur_items.loc[cur_items[cat_field] == cat, value_field], cat, ttest_alternative)\n",
    "        ttest(cur_items[value_field], \"all tokens\", ttest_alternative)\n",
    "\n",
    "\n",
    "value_attr = 'loss diff'\n",
    "ttest_alternative = 'two-sided'\n",
    "title = 'Loss Difference'\n",
    "\n",
    "\n",
    "# use the first name as the baseline by default\n",
    "baseline_name = names[0]\n",
    "model_names = ['LSTM Captioning']#list(filter(lambda name: name != baseline_name and 'contrastive' not in name.lower(), names))\n",
    "\n",
    "for split in ['val']:\n",
    "    print('Type level:')\n",
    "    token_items = split_items[split].token_items\n",
    "    analyze_value(\n",
    "        token_items[token_items['cnt'] >= 2],\n",
    "        value_attr,\n",
    "        model_names,\n",
    "        title='Type Level ' + title,\n",
    "        hlines=[0],\n",
    "        ttest_alternative=ttest_alternative,\n",
    "    )\n",
    "\n",
    "    all_token_items = split_items[split].all_token_items\n",
    "    if all_token_items is not None:\n",
    "        print('=' * 100)\n",
    "        print('Token level:')\n",
    "        analyze_value(\n",
    "            all_token_items.reset_index(drop=True),\n",
    "            value_attr,\n",
    "            model_names,\n",
    "            title='Token Level ' + title,\n",
    "            hlines=[0],\n",
    "            plotting=False,\n",
    "            ttest_alternative=ttest_alternative,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: 392\n",
      "animals         #: 99\n",
      "body_parts      #: 35\n",
      "clothing        #: 40\n",
      "food_drink      #: 74\n",
      "games_routines  #: 4\n",
      "household       #: 35\n",
      "places          #: 10\n",
      "toys            #: 49\n",
      "vehicles        #: 46\n",
      "plotting 392/392 = 100.00% items...\n",
      "saving plot P(subcategory | Noun Semantic Subcategories) Difference\n",
      "LSTM Captioning:\n",
      "plotting 392/392 = 100.00% items...\n",
      "saving plot LSTM Captioning P(subcategory | Noun Semantic Subcategories) Difference Distribution\n",
      "animals         #examples:    99 mean:  0.07 t-test result: statistic:   4.46 pvalue: 0.00\n",
      "body_parts      #examples:    35 mean:  0.01 t-test result: statistic:   0.23 pvalue: 0.82\n",
      "clothing        #examples:    40 mean:  0.04 t-test result: statistic:   2.11 pvalue: 0.04\n",
      "food_drink      #examples:    74 mean:  0.10 t-test result: statistic:   4.20 pvalue: 0.00\n",
      "games_routines  #examples:     4 mean:  0.22 t-test result: statistic:   1.99 pvalue: 0.14\n",
      "household       #examples:    35 mean:  0.07 t-test result: statistic:   3.16 pvalue: 0.00\n",
      "places          #examples:    10 mean:  0.03 t-test result: statistic:   0.59 pvalue: 0.57\n",
      "toys            #examples:    49 mean:  0.16 t-test result: statistic:   5.55 pvalue: 0.00\n",
      "vehicles        #examples:    46 mean:  0.06 t-test result: statistic:   1.84 pvalue: 0.07\n",
      "all tokens      #examples:   392 mean:  0.08 t-test result: statistic:   8.64 pvalue: 0.00\n",
      "#: 588\n",
      "intransitive    #: 281\n",
      "transitive      #: 307\n",
      "plotting 588/588 = 100.00% items...\n",
      "saving plot P(subcategory | Verb Transitivity SubCategories) Difference\n",
      "LSTM Captioning:\n",
      "plotting 588/588 = 100.00% items...\n",
      "saving plot LSTM Captioning P(subcategory | Verb Transitivity SubCategories) Difference Distribution\n",
      "intransitive    #examples:   281 mean:  0.01 t-test result: statistic:   1.56 pvalue: 0.12\n",
      "transitive      #examples:   307 mean:  0.03 t-test result: statistic:   5.11 pvalue: 0.00\n",
      "all tokens      #examples:   588 mean:  0.02 t-test result: statistic:   4.81 pvalue: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_subcat_value_diff(\n",
    "    all_items,\n",
    "    subcats,\n",
    "    names,\n",
    "    baseline_name=None,\n",
    "    model_names=None,\n",
    "    subcats_name='Subcategories',\n",
    "    subcat_name='subcategory',\n",
    "    word_name='word',\n",
    "    given_subcat=False,\n",
    "    plotting_subcats=False,\n",
    "    ttest_alternative='two-sided'\n",
    "):\n",
    "    # use the first name as the baseline by default\n",
    "    if baseline_name is None:\n",
    "        baseline_name = names[0]\n",
    "    if model_names is None:\n",
    "        model_names = list(filter(lambda name: name != baseline_name and 'contrastive' not in name.lower(), names))\n",
    "\n",
    "    subcat2word_ids = {\n",
    "        subcat_name: [word2idx[word] for word in subcat_words if word in word2idx]\n",
    "        for subcat_name, subcat_words in subcats.items()\n",
    "    }\n",
    "    all_subcat_words = list(filter(\n",
    "        lambda word: word in word2idx,\n",
    "        itertools.chain.from_iterable(subcat_words for subcat_name, subcat_words in subcats.items())))\n",
    "    all_subcat_word_ids = [word2idx[word] for word in all_subcat_words]\n",
    "\n",
    "    p_field = 'prob'\n",
    "    ps_field = 'probs'\n",
    "    #subcat_field = 'subcat'  # use global subcat_field\n",
    "\n",
    "    items = all_items[all_items[token_field].isin(all_subcat_words)].copy()\n",
    "    items[subcat_field] = items[token_field].map(word2subcat).astype('category')\n",
    "    print(f'#: {len(items)}')\n",
    "\n",
    "    p_cats_field = 'prob cats'\n",
    "    p_subcat_field = 'prob subcat'\n",
    "    p_subcat_given_cats_field = 'prob subcat given cats'\n",
    "    for name in names:\n",
    "        name_ps_field = f'{name} {ps_field}'\n",
    "        name_p_cats_field = f'{name} {p_cats_field}'\n",
    "        name_p_subcat_field = f'{name} {p_subcat_field}'\n",
    "        name_p_subcat_given_cats_field = f'{name} {p_subcat_given_cats_field}'\n",
    "        items[name_p_cats_field] = items[name_ps_field].map(lambda p: np.sum(p[all_subcat_word_ids]))\n",
    "        items[name_p_subcat_field] = items.apply(lambda item: np.sum(item[name_ps_field][subcat2word_ids[item[subcat_field]]]), axis=1)\n",
    "        items[name_p_subcat_given_cats_field] = items[name_p_subcat_field] / items[name_p_cats_field]\n",
    "        if name != baseline_name:\n",
    "            extend_items_value_diff(items, name, baseline_name, p_subcat_given_cats_field)\n",
    "    analyze_value(\n",
    "        items.reset_index(drop=True),\n",
    "        p_subcat_given_cats_field + ' diff',\n",
    "        model_names,\n",
    "        cat_field=subcat_field,\n",
    "        title=f'P({subcat_name} | {subcats_name}) Difference',\n",
    "        hlines=[0],\n",
    "        ttest_alternative=ttest_alternative,\n",
    "    )\n",
    "\n",
    "    if given_subcat:\n",
    "        for subcat_name, subcat_words in subcats.items():\n",
    "            items = all_items[all_items[token_field].isin(subcat_words)].copy()\n",
    "            items[token_field] = items[token_field].astype('category')\n",
    "            print(f'{subcat_name:14} #: {len(items)}')\n",
    "            palette = sns.color_palette('tab20')\n",
    "            palette = {word: palette[i % len(palette)] for i, word in enumerate(subcat_words)}\n",
    "            p_subcat_field = f'prob {subcat_name}'\n",
    "            p_given_subcat_field = f'prob given {subcat_name}'\n",
    "            for name in names:\n",
    "                name_p_field = f'{name} {p_field}'\n",
    "                name_ps_field = f'{name} {ps_field}'\n",
    "                name_p_subcat_field = f'{name} {p_subcat_field}'\n",
    "                name_p_given_subcat_field = f'{name} {p_given_subcat_field}'\n",
    "                items[name_p_subcat_field] = items[name_ps_field].map(lambda p: np.sum(p[subcat2word_ids[subcat_name]]))\n",
    "                items[name_p_given_subcat_field] = items[name_p_field] / items[name_p_subcat_field]\n",
    "                if name != baseline_name:\n",
    "                    extend_items_value_diff(items, name, baseline_name, p_given_subcat_field)\n",
    "            analyze_value(\n",
    "                items.reset_index(drop=True),\n",
    "                p_given_subcat_field + ' diff',\n",
    "                model_names,\n",
    "                cat_field=token_field,\n",
    "                palette=palette,\n",
    "                title=f'P({word_name} | {subcat_name}) Difference',\n",
    "                hlines=[0],\n",
    "                plotting=plotting_subcats,\n",
    "                ttest_alternative=ttest_alternative,\n",
    "            )\n",
    "\n",
    "\n",
    "subcats_name = {\n",
    "    'noun': 'Noun Semantic Subcategories',\n",
    "    'verb': 'Verb Transitivity SubCategories'\n",
    "}\n",
    "\n",
    "\n",
    "for split in ['val']:\n",
    "    for pos, subcats in pos_subcats.items():\n",
    "        items = split_items[split].all_token_items\n",
    "        items = items[items['syntactic category'].isin([pos])].copy()\n",
    "        analyze_subcat_value_diff(\n",
    "            items,\n",
    "            subcats,\n",
    "            names,\n",
    "            baseline_name=baseline_name,\n",
    "            model_names=model_names,\n",
    "            subcats_name=subcats_name[pos],\n",
    "            given_subcat=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM:\n",
      "Noun vs Verb:\n",
      "plotting 50/1596 = 3.13% items...\n",
      "saving plot LSTM Embedding Noun vs Verb t-SNE\n",
      "plotting 50/1596 = 3.13% items...\n",
      "saving plot LSTM Embedding Noun vs Verb PCA\n",
      "Semantic Categories:\n",
      "plotting 48/48 = 100.00% items...\n",
      "saving plot LSTM Embedding Semantic Categories t-SNE\n",
      "plotting 48/48 = 100.00% items...\n",
      "saving plot LSTM Embedding Semantic Categories PCA\n",
      "Verb Transitivity:\n",
      "plotting 50/50 = 100.00% items...\n",
      "saving plot LSTM Embedding Verb Transitivity t-SNE\n",
      "plotting 50/50 = 100.00% items...\n",
      "saving plot LSTM Embedding Verb Transitivity PCA\n",
      "LSTM Captioning:\n",
      "Noun vs Verb:\n",
      "plotting 50/1596 = 3.13% items...\n",
      "saving plot LSTM Captioning Embedding Noun vs Verb t-SNE\n",
      "plotting 50/1596 = 3.13% items...\n",
      "saving plot LSTM Captioning Embedding Noun vs Verb PCA\n",
      "Semantic Categories:\n",
      "plotting 48/48 = 100.00% items...\n",
      "saving plot LSTM Captioning Embedding Semantic Categories t-SNE\n",
      "plotting 48/48 = 100.00% items...\n",
      "saving plot LSTM Captioning Embedding Semantic Categories PCA\n",
      "Verb Transitivity:\n",
      "plotting 50/50 = 100.00% items...\n",
      "saving plot LSTM Captioning Embedding Verb Transitivity t-SNE\n",
      "plotting 50/50 = 100.00% items...\n",
      "saving plot LSTM Captioning Embedding Verb Transitivity PCA\n",
      "CBOW:\n",
      "Noun vs Verb:\n",
      "plotting 50/1596 = 3.13% items...\n",
      "saving plot CBOW Embedding Noun vs Verb t-SNE\n",
      "plotting 50/1596 = 3.13% items...\n",
      "saving plot CBOW Embedding Noun vs Verb PCA\n",
      "Semantic Categories:\n",
      "plotting 48/48 = 100.00% items...\n",
      "saving plot CBOW Embedding Semantic Categories t-SNE\n",
      "plotting 48/48 = 100.00% items...\n",
      "saving plot CBOW Embedding Semantic Categories PCA\n",
      "Verb Transitivity:\n",
      "plotting 50/50 = 100.00% items...\n",
      "saving plot CBOW Embedding Verb Transitivity t-SNE\n",
      "plotting 50/50 = 100.00% items...\n",
      "saving plot CBOW Embedding Verb Transitivity PCA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x2100 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_field = \"syntactic category\"\n",
    "\n",
    "baseline_name = names[0]\n",
    "\n",
    "item_combinations = lambda split: {\n",
    "#   \"All\":                 (split_all_pos_items[split], None),\n",
    "#   \"Most Frequent Words\": (split_all_pos_items[split], len(used_poses) * interleaving_step * (25 // interleaving_step)),\n",
    "#   \"All Noun vs Verb\":    (split_pos_pos_items[split][('noun', 'verb')], None),\n",
    "    \"Noun vs Verb\":        (split_pos_pos_items[split][('noun', 'verb')], 2 * interleaving_step * (25 // interleaving_step)),\n",
    "    \"Semantic Categories\": (get_subcat_items(split_pos_items[split]['noun'], 'noun'), 10000),\n",
    "    \"Verb Transitivity\":   (get_subcat_items(split_pos_items[split]['verb'], 'verb'), 10000),\n",
    "}\n",
    "\n",
    "for split in ['train']:\n",
    "    for name in names:\n",
    "        print(f'{name}:')\n",
    "        loss_field = f'{name} loss'\n",
    "        loss_diff_field = f'{name} loss diff'\n",
    "        fields = [pos_field, \"cnt\", \"logcnt\", conc_field, \"AnimPhysical\", \"AnimMental\", \"Category\", \"AoA\", loss_field, loss_diff_field]\n",
    "\n",
    "        for items_name, (items, n_items) in item_combinations(split).items():\n",
    "            print(f'{items_name}:')\n",
    "            token_kwargs = {'fontsize': 'small'} if n_items is not None else None\n",
    "            pos_hue = subcat_field if items_name in [\"Semantic Categories\", \"Verb Transitivity\"] else pos_field\n",
    "            items = items.copy()\n",
    "            items[pos_hue] = items[pos_hue].cat.remove_unused_categories()\n",
    "\n",
    "            title_prefix = f\"{name} {repres_name} {items_name} \"\n",
    "            for hue, x, y, xlabel, ylabel, axis_option, title, plot_reg in [\n",
    "                (pos_hue, f\"{name} tsne 0\", f\"{name} tsne 1\", \"\", \"\", \"off\", title_prefix + \"t-SNE\", False),\n",
    "                #(pos_hue, f\"{name} pca 0\", f\"{name} pca 1\", \"principal component 1\", \"principal component 2\", \"on\", title_prefix + \"PCA\", False),\n",
    "                #(\"logcnt\", f\"{name} pca 0\", f\"{name} pca 1\", \"principal component 1\", \"principal component 2\", \"on\", title_prefix + \"PCA\", False),\n",
    "                #(pos_hue, f\"{name} pca 0\", \"logcnt\", \"principal component 1\", \"log frequency\", \"on\", title_prefix + \"Correlation between principal component 1 and log frequency\", True),\n",
    "                (pos_hue, f\"{name} pca 1\", f\"{name} pca 2\", \"principal component 2\", \"principal component 3\", \"on\", title_prefix + \"PCA\", False),\n",
    "            ]:\n",
    "                kwargs = {\n",
    "                    key: globals()[key]\n",
    "                    for key in ['x', 'y', 'n_items', 'token_kwargs', 'xlabel', 'ylabel', 'axis_option', 'title', 'figsize']\n",
    "                }\n",
    "                plot(sns.scatterplot, items, hue=hue, **kwargs)\n",
    "                output_fig(title)\n",
    "                if plot_reg:\n",
    "                    plot(sns.regplot, items, **kwargs)\n",
    "                    output_fig(title + ' regression')\n",
    "\n",
    "for split in ['val'][:0]:\n",
    "    for name in filter(lambda name: name != baseline_name, names):\n",
    "        print(f'{name}:')\n",
    "        loss_field = f'{name} loss'\n",
    "        loss_diff_field = f'{name} loss diff'\n",
    "\n",
    "        item_combinations_split = item_combinations(split)\n",
    "        item_combinations_split = item_combinations_split[1:2] + item_combinations_split[4:6]\n",
    "        for items_name, (items, n_items) in item_combinations_split.items():\n",
    "            figname_prefix = f'{name} {items_name} '\n",
    "            print(f'{items_name}:')\n",
    "            token_kwargs = {'fontsize': 'small'} if n_items is not None else None\n",
    "            pos_hue = subcat_field if items_name in [\"Semantic Categories\", \"Verb Transitivity\"] else pos_field\n",
    "\n",
    "            loss_diff_items = items.sort_values(loss_diff_field)\n",
    "            if n_items is not None:\n",
    "                loss_diff_items = loss_diff_items[loss_diff_items['cnt'] >= 5]\n",
    "                if n_items * 2 >= len(loss_diff_items):\n",
    "                    n_items = len(loss_diff_items)\n",
    "            plot(sns.scatterplot, loss_diff_items, x=conc_field, y=loss_diff_field, hue=pos_hue, n_items=n_items, hlines=[0], token_kwargs=token_kwargs, title=\"vs\", figsize=figsize)\n",
    "            output_fig(figname_prefix + f'Concreteness vs loss diff {n_items} words')\n",
    "            if n_items is None and False:\n",
    "                plot(sns.regplot, loss_diff_items, x=conc_field, y=loss_diff_field, n_items=n_items, hlines=[0], token_kwargs=token_kwargs, title=\"vs\", figsize=figsize)\n",
    "                output_fig(figname_prefix + f'Concrenteness vs loss diff all words')\n",
    "                #plot(sns.catplot, loss_diff_items, x=pos_field, y=loss_diff_field, n_items=n_items, color=\"b\", hlines=[0], title=\"vs\", figsize=figsize,) #kind=\"violin\", inner=\"stick\",\n",
    "                #output_Fig(figname_prefix + f'{pos_field} vs loss diff all words')\n",
    "            if n_items is not None and n_items < len(loss_diff_items):\n",
    "                print('highest:')\n",
    "                plot(sns.scatterplot, loss_diff_items[::-1], x=conc_field, y=loss_diff_field, hue=pos_hue, n_items=n_items, hlines=[0], token_kwargs=token_kwargs, title=\"vs\", figsize=figsize)\n",
    "                output_fig(figname_prefix + 'Concreness vs loss diff highest words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
